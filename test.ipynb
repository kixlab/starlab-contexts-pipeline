{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking out the \"instructions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "from helpers.bert import bert_embedding, tfidf_embedding  \n",
    "from helpers.bert import hierarchical_clustering, clustering_custom\n",
    "\n",
    "import json\n",
    "\n",
    "task_id = \"test-carbonara\"  # Replace with your actual task_id\n",
    "# task_id = \"tech-photoshop\"\n",
    "video_file_path = f'static/results/{task_id}/video_data.json'\n",
    "taxonomies_file_path = f'static/results/{task_id}/taxonomies.json'\n",
    "partial_taxonomies_file_path = f'static/results/{task_id}/partial_taxonomies.json'\n",
    "\n",
    "with open(video_file_path, 'r') as file:\n",
    "    video_data = json.load(file)\n",
    "\n",
    "with open(taxonomies_file_path, 'r') as file:\n",
    "    taxonomies_data = json.load(file)\n",
    "\n",
    "if task_id == \"carbonara\":\n",
    "    with open(partial_taxonomies_file_path, 'r') as file:\n",
    "        partial_taxonomies_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chop up some bacon.\n",
      "Grate pecorino romano cheese into a bowl.\n",
      "Add pepper to the grated cheese.\n",
      "Salt the boiling water.\n",
      "Add spaghetti to the salted water.\n",
      "Cook the bacon until crispy.\n",
      "Turn off the burner and let the bacon cool.\n",
      "Add a bit of pasta water to the sauce ingredients.\n",
      "Transfer noodles directly from the water into the bacon pan.\n",
      "Add the sauce to the noodles in the pan.\n",
      "Finish with more pepper and pecorino or parmesan cheese.\n",
      "\n",
      "Slice half a pound of thick cut bacon into three quarters of an inch pieces.\n",
      "Turn the pan on to medium heat.\n",
      "Add a tiny bit of olive oil to the pan.\n",
      "Break the bacon into the pan.\n",
      "Render the fat from the bacon until it is nice and crispy.\n",
      "Season the pasta water with about a tablespoon of kosher salt in four quarts of water.\n",
      "Grate about three ounces each of Pecorino Romano and Parmigiano Reggiano cheese until fine.\n",
      "Whisk together three whole eggs and two egg yolks in a bowl.\n",
      "Add the grated cheese to the egg mixture and whisk until combined.\n",
      "Add fresh ground black pepper to the egg and cheese mixture and whisk it in.\n",
      "Drain the bacon on a paper towel for garnish.\n",
      "Drain off most of the bacon fat, leaving about two tablespoons in the pan.\n",
      "Cook spaghetti in boiling water for about 10 minutes until slightly al dente.\n",
      "Use tongs to transfer the cooked spaghetti into the pan with the bacon fat.\n",
      "Add the egg and cheese mixture to the spaghetti and mix gently.\n",
      "If the sauce is too thick, add a little bit of reserved pasta water to loosen it up.\n",
      "Plate the pasta and top with additional grated Parmigiano Reggiano and crispy bacon.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def likeliness_to_continuation(instruction1, instruction2):\n",
    "    embeddings = bert_embedding([instruction1, instruction2])\n",
    "    cos_sim = np.dot(embeddings[0], embeddings[1]) / (np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1]))\n",
    "\n",
    "videos = []\n",
    "for video in video_data:\n",
    "    videos.append({\n",
    "        \"subtitles\": video[\"sentences\"],\n",
    "        \"video_id\": video[\"video_id\"],\n",
    "        \"steps\": video[\"steps\"]\n",
    "    })\n",
    "for video in videos:\n",
    "    ### print top-5 cosine similarities for instructions\n",
    "    embeddings = bert_embedding([subtitle[\"text\"] for subtitle in video[\"subtitles\"]])\n",
    "    prev_instruction = \"\"\n",
    "    for step in video[\"steps\"]:\n",
    "        instruction = step[\"instruction\"]\n",
    "        outcome = step[\"expected_outcome\"]\n",
    "        start_index = 0\n",
    "        end_index = 0\n",
    "        if 'start_index' in step:\n",
    "            start_index = step[\"start_index\"]\n",
    "        else:\n",
    "            start_index = step['start']\n",
    "        if 'end_index' in step:\n",
    "            end_index = step[\"end_index\"]\n",
    "        else:\n",
    "            end_index = step['end']\n",
    "        instruction_embedding = bert_embedding(instruction)\n",
    "        top_5 = []\n",
    "        matched_sims = []\n",
    "        for i, subtitle in enumerate(video[\"subtitles\"]):\n",
    "            subtitle_embedding = embeddings[i]\n",
    "            cur_sim = np.dot(instruction_embedding, subtitle_embedding) / (np.linalg.norm(instruction_embedding) * np.linalg.norm(subtitle_embedding))\n",
    "            if i >= start_index and i <= end_index:\n",
    "                matched_sims.append((cur_sim, i))\n",
    "            if len(top_5) < 5:\n",
    "                top_5.append((cur_sim, i))\n",
    "            else:\n",
    "                top_5.sort()\n",
    "                if cur_sim > top_5[0][0]:\n",
    "                    top_5[0] = (cur_sim, i)\n",
    "        score = likeliness_to_continuation(prev_instruction, instruction)\n",
    "        prev_instruction = instruction\n",
    "        # print(round(score, 2))\n",
    "        print(f\"{instruction}\")\n",
    "        # for sim, i in matched_sims:\n",
    "        #     print(f\"index: {i}, similarity: {sim}, subtitle: {video['subtitles'][i]['text']}\")\n",
    "        # print()\n",
    "        # print(f\"Top-5:\")\n",
    "        # top_5.sort(reverse=True)\n",
    "        # for sim, i in top_5:\n",
    "        #     print(f\"index: {i}, similarity: {sim}, subtitle: {video['subtitles'][i]['text']}\")\n",
    "        # print()\n",
    "        # print(\"-------------------------------------------------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiments with the embeddings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def plot_dendrogram(model, dictionary, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "\n",
    "    def llf(id):\n",
    "        if id < n_samples:\n",
    "            return dictionary[id][\"variation_index\"]\n",
    "        else:\n",
    "            return '[%d %d %d]' % (linkage_matrix[id, 0], linkage_matrix[id, 1], linkage_matrix[id, 2])\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, leaf_label_func=llf, **kwargs)\n",
    "\n",
    "\n",
    "def cluster_similar_steps(variations, embedding_f=bert_embedding):\n",
    "    \"\"\"\n",
    "    cluster similar steps together\n",
    "    \"\"\"\n",
    "\n",
    "    linearized = [step for variation in variations for step in variation]\n",
    "\n",
    "    N = len(linearized)\n",
    "\n",
    "    embeddings = embedding_f([step[\"description\"] for step in linearized])\n",
    "\n",
    "    # sim_matrix = np.zeros((N, N))\n",
    "    # for i in range(N):\n",
    "    #     for j in range(i+1, N):\n",
    "    #         sim_matrix[i][j] = np.dot(embeddings[i], embeddings[j])\n",
    "    #         sim_matrix[j][i] = sim_matrix[i][j]\n",
    "    \n",
    "    clustering = AgglomerativeClustering(\n",
    "        n_clusters=None,\n",
    "        distance_threshold=0,\n",
    "        linkage='average',\n",
    "        metric='cosine'\n",
    "    ).fit(embeddings)\n",
    "\n",
    "    plot_dendrogram(clustering, linearized, truncate_mode='level', p=10)\n",
    "    \n",
    "    clusters = {}\n",
    "    for i, cluster_id in enumerate(clustering.labels_):\n",
    "        if cluster_id not in clusters:\n",
    "            clusters[cluster_id] = []\n",
    "        clusters[cluster_id].append(linearized[i])\n",
    "    return clusters\n",
    "\n",
    "def dendogramm(data):\n",
    "    variations = []\n",
    "    for idx, tax in enumerate(data):\n",
    "        steps = tax[\"steps\"]\n",
    "        variations.append([\n",
    "            {\n",
    "                \"description\": step[\"description\"],\n",
    "                \"variation_index\": str(idx+1) + \"-\" + str(step_idx+1)\n",
    "            }\n",
    "            for step_idx, step in enumerate(steps)\n",
    "        ])\n",
    "        \n",
    "    result = cluster_similar_steps(variations)\n",
    "\n",
    "    for kk, vv in result.items():\n",
    "        print(\"Cluster\", kk)\n",
    "        for step in vv:\n",
    "            print(step[\"variation_index\"], step[\"description\"])\n",
    "        print()\n",
    "\n",
    "def t_sne(n_classes, text_per_class, embedding_f=bert_embedding):\n",
    "    \"\"\"\n",
    "    t-SNE for clustering the embeddings of text of N classes\n",
    "    color each class with a different color (e.g., red, blue, green, etc.)\n",
    "    \"\"\"\n",
    "    color_per_class = []\n",
    "    for i in range(n_classes):\n",
    "        gap = 255 // (n_classes)\n",
    "        val = min(round((i * gap) / 225, 2), 1)\n",
    "        color_per_class.append((val, 1-val, 1-val))\n",
    "\n",
    "    all_texts = []\n",
    "    for i in range(n_classes):\n",
    "        all_texts.extend(text_per_class[i])\n",
    "    \n",
    "    all_embeddings = embedding_f(all_texts)\n",
    "\n",
    "    embeddings_per_class = []\n",
    "    l = 0\n",
    "    for i in range(n_classes):\n",
    "        r = l + len(text_per_class[i])\n",
    "        embeddings_per_class.append(all_embeddings[l:r])\n",
    "        l = r\n",
    "        \n",
    "    embeddings_2d = np.concatenate(embeddings_per_class)\n",
    "    # print(\"Initial:\", embeddings_2d.shape)\n",
    "    \n",
    "    ## reduce dimensionality to (n, 50) using PCA\n",
    "    min_dim = min(50, embeddings_2d.shape[1], embeddings_2d.shape[0])\n",
    "    pca = PCA(n_components=min_dim)\n",
    "    embeddings_2d = pca.fit_transform(embeddings_2d)\n",
    "    # print(\"PCA-ed:\", embeddings_2d.shape)\n",
    "\n",
    "    # t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    embeddings_2d = tsne.fit_transform(embeddings_2d)\n",
    "    # print(\"t-SNE-ed:\", embeddings_2d.shape)\n",
    "    \n",
    "    # KernelPCA\n",
    "    # from sklearn.decomposition import KernelPCA\n",
    "    # kpca = KernelPCA(n_components=2, kernel='linear', gamma=15)\n",
    "    # embeddings_2d = kpca.fit_transform(embeddings_2d)\n",
    "    # print(\"KernelPCA-ed:\", embeddings_2d.shape)\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    l = 0\n",
    "    for i in range(n_classes):\n",
    "        r = l + len(text_per_class[i])\n",
    "        plt.scatter(embeddings_2d[l:r, 0], embeddings_2d[l:r, 1], color=color_per_class[i], label=str(i))\n",
    "        l = r\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def get_most_differing_examples_based_on_pca(n_classes, text_per_class, embedding_f=bert_embedding, dims=4):\n",
    "    all_texts = []\n",
    "    for i in range(n_classes):\n",
    "        all_texts.extend(text_per_class[i])\n",
    "\n",
    "    print(\"All texts:\", len(all_texts))\n",
    "    \n",
    "    all_embeddings = embedding_f(all_texts)\n",
    "        \n",
    "    embeddings_2d = np.array(all_embeddings)\n",
    "    pca = PCA(n_components=dims)\n",
    "    embeddings_2d = pca.fit_transform(embeddings_2d)\n",
    "    \n",
    "    diffs_per_dim = []\n",
    "    for i in range(dims):\n",
    "        sorted_items = sorted(enumerate(embeddings_2d[:, i]), key=lambda x: x[1])\n",
    "        sorted_items = [x[0] for x in sorted_items]\n",
    "        cur_diffs = [all_texts[sorted_items[i]] for i in range(len(sorted_items))]\n",
    "        diffs_per_dim.append(cur_diffs)\n",
    "    \n",
    "    for i in range(dims):\n",
    "        print(\"Dimension\", i)\n",
    "        for j in diffs_per_dim[i]:\n",
    "            print(j)\n",
    "        print()\n",
    "\n",
    "def taxonomy_low(texts, method_idx=0):\n",
    "    METHODS = [\n",
    "        {\n",
    "            \"label\": \"bert\",\n",
    "            \"func\": bert_embedding,\n",
    "            \"linkage\": \"complete\",\n",
    "        },\n",
    "        {\n",
    "            \"label\": \"tfidf\",\n",
    "            \"func\": tfidf_embedding,\n",
    "            \"linkage\": \"average\",\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # ELEMENTS = [\"step\"]\n",
    "    # SIMS = [\"5\", \"7\", \"9\"]\n",
    "    # for element in ELEMENTS:\n",
    "    #     print(\"##\", element)\n",
    "    #     text_per_class = []\n",
    "    #     for sim in SIMS:\n",
    "    #         text_per_class.append([])\n",
    "    #         cur_key = element + \"_\" + sim\n",
    "    #         # print(\"-\", cur_key)\n",
    "    #         for elem in data[cur_key]:\n",
    "    #             # print(\"\\t-\", elem[\"description\"])\n",
    "    #             text_per_class[-1].append(elem[\"description\"])\n",
    "    #     t_sne(len(SIMS), text_per_class)\n",
    "\n",
    "\n",
    "    method = METHODS[method_idx]\n",
    "    # sim_thresh = 0.3    \n",
    "    # labels = clustering_custom(texts, sim_thresh, embedding_method=method[\"label\"])\n",
    "    labels = hierarchical_clustering(texts, embedding_method=method[\"label\"], linkage=method['linkage'], distance_threshold=0.2)\n",
    "    \n",
    "    text_classes = {}\n",
    "    for i, label in enumerate(labels):\n",
    "        if label not in text_classes:\n",
    "            text_classes[label] = []\n",
    "        text_classes[label].append(texts[i])\n",
    "\n",
    "    get_most_differing_examples_based_on_pca(len(text_classes), list(text_classes.values()), embedding_f=method[\"func\"], dims=4)\n",
    "\n",
    "    text_classes = {k: v for k, v in text_classes.items() if len(v) > 1}\n",
    "    print(json.dumps(text_classes, indent=4))\n",
    "    ### delete classes that have less than 2 elements\n",
    "    \n",
    "    t_sne(len(text_classes), list(text_classes.values()), embedding_f=method[\"func\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 26:\n",
      "\tSingle element\n",
      "Cluster 22:\n",
      "\tSingle element\n",
      "Cluster 20:\n",
      "\tSingle element\n",
      "Cluster 17:\n",
      "\tSingle element\n",
      "Cluster 18:\n",
      "\tSingle element\n",
      "Cluster 24:\n",
      "\tSingle element\n",
      "Cluster 14:\n",
      "\tSingle element\n",
      "Cluster 0:\n",
      "\tMin dist: 0.8527458906173706\n",
      "Cluster 21:\n",
      "\tSingle element\n",
      "Cluster 19:\n",
      "\tSingle element\n",
      "Cluster 10:\n",
      "\tSingle element\n",
      "Cluster 25:\n",
      "\tSingle element\n",
      "Cluster 12:\n",
      "\tSingle element\n",
      "Cluster 23:\n",
      "\tSingle element\n",
      "Cluster 9:\n",
      "\tSingle element\n",
      "Cluster 16:\n",
      "\tSingle element\n",
      "Cluster 15:\n",
      "\tSingle element\n",
      "Cluster 13:\n",
      "\tSingle element\n",
      "Cluster 6:\n",
      "\tSingle element\n",
      "Cluster 11:\n",
      "\tSingle element\n",
      "Cluster 5:\n",
      "\tSingle element\n",
      "Cluster 7:\n",
      "\tSingle element\n",
      "Cluster 8:\n",
      "\tSingle element\n",
      "Cluster 4:\n",
      "\tSingle element\n",
      "Cluster 2:\n",
      "\tSingle element\n",
      "Cluster 3:\n",
      "\tSingle element\n",
      "Cluster 1:\n",
      "\tSingle element\n",
      "All texts: 28\n",
      "Dimension 0\n",
      "Add a bit of pasta water to the sauce ingredients.\n",
      "Season the pasta water with about a tablespoon of kosher salt in four quarts of water.\n",
      "If the sauce is too thick, add a little bit of reserved pasta water to loosen it up.\n",
      "Add the egg and cheese mixture to the spaghetti and mix gently.\n",
      "Grate pecorino romano cheese into a bowl.\n",
      "Grate about three ounces each of Pecorino Romano and Parmigiano Reggiano cheese until fine.\n",
      "Add spaghetti to the salted water.\n",
      "Add pepper to the grated cheese.\n",
      "Add the sauce to the noodles in the pan.\n",
      "Finish with more pepper and pecorino or parmesan cheese.\n",
      "Cook spaghetti in boiling water for about 10 minutes until slightly al dente.\n",
      "Add the grated cheese to the egg mixture and whisk until combined.\n",
      "Add fresh ground black pepper to the egg and cheese mixture and whisk it in.\n",
      "Plate the pasta and top with additional grated Parmigiano Reggiano and crispy bacon.\n",
      "Whisk together three whole eggs and two egg yolks in a bowl.\n",
      "Salt the boiling water.\n",
      "Add a tiny bit of olive oil to the pan.\n",
      "Transfer noodles directly from the water into the bacon pan.\n",
      "Turn the pan on to medium heat.\n",
      "Use tongs to transfer the cooked spaghetti into the pan with the bacon fat.\n",
      "Slice half a pound of thick cut bacon into three quarters of an inch pieces.\n",
      "Drain the bacon on a paper towel for garnish.\n",
      "Chop up some bacon.\n",
      "Turn off the burner and let the bacon cool.\n",
      "Drain off most of the bacon fat, leaving about two tablespoons in the pan.\n",
      "Render the fat from the bacon until it is nice and crispy.\n",
      "Break the bacon into the pan.\n",
      "Cook the bacon until crispy.\n",
      "\n",
      "Dimension 1\n",
      "Add the sauce to the noodles in the pan.\n",
      "Cook spaghetti in boiling water for about 10 minutes until slightly al dente.\n",
      "If the sauce is too thick, add a little bit of reserved pasta water to loosen it up.\n",
      "Add spaghetti to the salted water.\n",
      "Turn the pan on to medium heat.\n",
      "Transfer noodles directly from the water into the bacon pan.\n",
      "Add a bit of pasta water to the sauce ingredients.\n",
      "Add a tiny bit of olive oil to the pan.\n",
      "Use tongs to transfer the cooked spaghetti into the pan with the bacon fat.\n",
      "Salt the boiling water.\n",
      "Season the pasta water with about a tablespoon of kosher salt in four quarts of water.\n",
      "Add the egg and cheese mixture to the spaghetti and mix gently.\n",
      "Turn off the burner and let the bacon cool.\n",
      "Drain off most of the bacon fat, leaving about two tablespoons in the pan.\n",
      "Break the bacon into the pan.\n",
      "Cook the bacon until crispy.\n",
      "Drain the bacon on a paper towel for garnish.\n",
      "Chop up some bacon.\n",
      "Render the fat from the bacon until it is nice and crispy.\n",
      "Plate the pasta and top with additional grated Parmigiano Reggiano and crispy bacon.\n",
      "Slice half a pound of thick cut bacon into three quarters of an inch pieces.\n",
      "Whisk together three whole eggs and two egg yolks in a bowl.\n",
      "Add fresh ground black pepper to the egg and cheese mixture and whisk it in.\n",
      "Add the grated cheese to the egg mixture and whisk until combined.\n",
      "Add pepper to the grated cheese.\n",
      "Finish with more pepper and pecorino or parmesan cheese.\n",
      "Grate pecorino romano cheese into a bowl.\n",
      "Grate about three ounces each of Pecorino Romano and Parmigiano Reggiano cheese until fine.\n",
      "\n",
      "Dimension 2\n",
      "Plate the pasta and top with additional grated Parmigiano Reggiano and crispy bacon.\n",
      "Cook spaghetti in boiling water for about 10 minutes until slightly al dente.\n",
      "Use tongs to transfer the cooked spaghetti into the pan with the bacon fat.\n",
      "Slice half a pound of thick cut bacon into three quarters of an inch pieces.\n",
      "Add spaghetti to the salted water.\n",
      "Chop up some bacon.\n",
      "Season the pasta water with about a tablespoon of kosher salt in four quarts of water.\n",
      "Grate about three ounces each of Pecorino Romano and Parmigiano Reggiano cheese until fine.\n",
      "Render the fat from the bacon until it is nice and crispy.\n",
      "Drain the bacon on a paper towel for garnish.\n",
      "Transfer noodles directly from the water into the bacon pan.\n",
      "Finish with more pepper and pecorino or parmesan cheese.\n",
      "Grate pecorino romano cheese into a bowl.\n",
      "Cook the bacon until crispy.\n",
      "If the sauce is too thick, add a little bit of reserved pasta water to loosen it up.\n",
      "Add a bit of pasta water to the sauce ingredients.\n",
      "Drain off most of the bacon fat, leaving about two tablespoons in the pan.\n",
      "Add the egg and cheese mixture to the spaghetti and mix gently.\n",
      "Add pepper to the grated cheese.\n",
      "Break the bacon into the pan.\n",
      "Add the sauce to the noodles in the pan.\n",
      "Turn off the burner and let the bacon cool.\n",
      "Salt the boiling water.\n",
      "Add the grated cheese to the egg mixture and whisk until combined.\n",
      "Add fresh ground black pepper to the egg and cheese mixture and whisk it in.\n",
      "Whisk together three whole eggs and two egg yolks in a bowl.\n",
      "Add a tiny bit of olive oil to the pan.\n",
      "Turn the pan on to medium heat.\n",
      "\n",
      "Dimension 3\n",
      "Salt the boiling water.\n",
      "Add a tiny bit of olive oil to the pan.\n",
      "Grate about three ounces each of Pecorino Romano and Parmigiano Reggiano cheese until fine.\n",
      "Drain the bacon on a paper towel for garnish.\n",
      "Turn the pan on to medium heat.\n",
      "Add pepper to the grated cheese.\n",
      "Finish with more pepper and pecorino or parmesan cheese.\n",
      "Turn off the burner and let the bacon cool.\n",
      "Grate pecorino romano cheese into a bowl.\n",
      "If the sauce is too thick, add a little bit of reserved pasta water to loosen it up.\n",
      "Add a bit of pasta water to the sauce ingredients.\n",
      "Season the pasta water with about a tablespoon of kosher salt in four quarts of water.\n",
      "Drain off most of the bacon fat, leaving about two tablespoons in the pan.\n",
      "Add spaghetti to the salted water.\n",
      "Break the bacon into the pan.\n",
      "Cook the bacon until crispy.\n",
      "Render the fat from the bacon until it is nice and crispy.\n",
      "Chop up some bacon.\n",
      "Add the sauce to the noodles in the pan.\n",
      "Cook spaghetti in boiling water for about 10 minutes until slightly al dente.\n",
      "Plate the pasta and top with additional grated Parmigiano Reggiano and crispy bacon.\n",
      "Transfer noodles directly from the water into the bacon pan.\n",
      "Add fresh ground black pepper to the egg and cheese mixture and whisk it in.\n",
      "Add the grated cheese to the egg mixture and whisk until combined.\n",
      "Use tongs to transfer the cooked spaghetti into the pan with the bacon fat.\n",
      "Slice half a pound of thick cut bacon into three quarters of an inch pieces.\n",
      "Add the egg and cheese mixture to the spaghetti and mix gently.\n",
      "Whisk together three whole eggs and two egg yolks in a bowl.\n",
      "\n",
      "{\n",
      "    \"0\": [\n",
      "        \"Add a bit of pasta water to the sauce ingredients.\",\n",
      "        \"If the sauce is too thick, add a little bit of reserved pasta water to loosen it up.\"\n",
      "    ]\n",
      "}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "perplexity must be less than n_samples",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m         instruction \u001b[38;5;241m=\u001b[39m step[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstruction\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      5\u001b[0m         texts\u001b[38;5;241m.\u001b[39mappend(instruction)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mtaxonomy_low\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 211\u001b[0m, in \u001b[0;36mtaxonomy_low\u001b[0;34m(texts, method_idx)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mdumps(text_classes, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m### delete classes that have less than 2 elements\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m \u001b[43mt_sne\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_classes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_classes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_f\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 119\u001b[0m, in \u001b[0;36mt_sne\u001b[0;34m(n_classes, text_per_class, embedding_f)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# print(\"PCA-ed:\", embeddings_2d.shape)\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# t-SNE\u001b[39;00m\n\u001b[1;32m    118\u001b[0m tsne \u001b[38;5;241m=\u001b[39m TSNE(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 119\u001b[0m embeddings_2d \u001b[38;5;241m=\u001b[39m \u001b[43mtsne\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings_2d\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# print(\"t-SNE-ed:\", embeddings_2d.shape)\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# KernelPCA\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# embeddings_2d = kpca.fit_transform(embeddings_2d)\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# print(\"KernelPCA-ed:\", embeddings_2d.shape)\u001b[39;00m\n\u001b[1;32m    129\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "File \u001b[0;32m~/.conda/envs/starlab-video-analysis/lib/python3.10/site-packages/sklearn/utils/_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    301\u001b[0m         )\n",
      "File \u001b[0;32m~/.conda/envs/starlab-video-analysis/lib/python3.10/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/starlab-video-analysis/lib/python3.10/site-packages/sklearn/manifold/_t_sne.py:1135\u001b[0m, in \u001b[0;36mTSNE.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(\n\u001b[1;32m   1111\u001b[0m     \u001b[38;5;66;03m# TSNE.metric is not validated yet\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m     prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1113\u001b[0m )\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit X into an embedded space and return that transformed output.\u001b[39;00m\n\u001b[1;32m   1116\u001b[0m \n\u001b[1;32m   1117\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;124;03m        Embedding of the training data in low-dimensional space.\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1135\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_params_vs_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1136\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X)\n\u001b[1;32m   1137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_ \u001b[38;5;241m=\u001b[39m embedding\n",
      "File \u001b[0;32m~/.conda/envs/starlab-video-analysis/lib/python3.10/site-packages/sklearn/manifold/_t_sne.py:846\u001b[0m, in \u001b[0;36mTSNE._check_params_vs_input\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_params_vs_input\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mperplexity \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 846\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperplexity must be less than n_samples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: perplexity must be less than n_samples"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "for video in videos:\n",
    "    for step in video[\"steps\"]:\n",
    "        instruction = step[\"instruction\"]\n",
    "        texts.append(instruction)\n",
    "    \n",
    "taxonomy_low(texts, method_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using large-scale datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import json\n",
    "\n",
    "dataset_maps = {\n",
    "    \"zeqianli/HowToStep\": {\n",
    "        \"path\": \"./static/datasets/howtostep\",\n",
    "        \"split\": \"train\",\n",
    "    },\n",
    "    \"ajibawa-2023/WikiHow\": {\n",
    "        \"path\": \"./static/datasets/wikihow\",\n",
    "        \"split\": \"train\",\n",
    "    },\n",
    "    \"ht-step\": {\n",
    "        \"path\": \"./static/datasets/ht-step\",\n",
    "        \"split\": \"train\",\n",
    "    }\n",
    "}\n",
    "\n",
    "def setup_ht_step(dataset_info):\n",
    "    \"\"\"\n",
    "    return a dictionary --> \n",
    "    activity: {\n",
    "        variation_index,\n",
    "        variation,\n",
    "        steps: [{\n",
    "            step_id,\n",
    "            step_index,\n",
    "            headline,\n",
    "            paragraph,\n",
    "        }]\n",
    "        videos: [{\n",
    "            video_id,\n",
    "            variation,\n",
    "            segments: [{\n",
    "                segment,\n",
    "                partial,\n",
    "                step_id,\n",
    "            }]\n",
    "        }]\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    path = dataset_info[\"path\"]\n",
    "    split = dataset_info[\"split\"]\n",
    "    # video_splits = json.load(open(path + \"/video_splits.json\"))\n",
    "    taxonomy = pd.read_csv(dataset_info[\"path\"] + \"/taxonomy.csv\")\n",
    "    ants = json.load(open(dataset_info[\"path\"] + \"/annotations.json\"))\n",
    "    # -- get the annotations for this split\n",
    "    ants = {kk:vv for kk,vv in ants.items() if vv['subset'] == split}\n",
    "\n",
    "    step2info = {vv['global_step_index']: vv for vv in taxonomy.to_dict(orient='records')}\n",
    "    act2vars2steps = {}\n",
    "    for kk, vv in step2info.items():\n",
    "        activity = vv[\"activity\"]\n",
    "        variation = vv[\"variation\"]\n",
    "        if activity not in act2vars2steps:\n",
    "            act2vars2steps[activity] = {}\n",
    "        if variation not in act2vars2steps[activity]:\n",
    "            act2vars2steps[activity][variation] = []\n",
    "        act2vars2steps[activity][variation].append(vv)\n",
    "    for act, vv in act2vars2steps.items():\n",
    "        vars_list = []\n",
    "        for var, steps in vv.items():\n",
    "            vars_list.append({\n",
    "                \"variation\": var,\n",
    "                \"steps\": sorted(steps, key=lambda x: -x[\"step_index\"])\n",
    "            })\n",
    "\n",
    "    act_list = [{\n",
    "        \"task\": act,\n",
    "        \"variations\": vv,\n",
    "    } for act, vv in act2vars2steps.items()]\n",
    "    act_list = sorted(act_list, key=lambda x: -len(x[\"variations\"]))\n",
    "\n",
    "    for kk, vv in ants.items():\n",
    "        activity = vv[\"activity\"]\n",
    "        variation = vv[\"variation\"]\n",
    "        for seg in vv[\"annotations\"]:\n",
    "            cur_step_info = step2info[seg[\"id\"]]\n",
    "            assert cur_step_info[\"global_step_index\"] == seg[\"id\"] and cur_step_info[\"activity\"] == activity and cur_step_info[\"variation\"] == variation\n",
    "\n",
    "    return {\n",
    "        \"annotations\": ants,\n",
    "        \"step2info\": step2info,\n",
    "        \"activity_info\": act_list,\n",
    "    }\n",
    "\n",
    "def get_dataset(dataset_id):\n",
    "    dataset = None\n",
    "    ### check if dataset is already downloaded\n",
    "    if dataset_id in dataset_maps:\n",
    "        if dataset_id == \"ht-step\":\n",
    "            return setup_ht_step(dataset_maps[dataset_id])\n",
    "        path = dataset_maps[dataset_id][\"path\"]\n",
    "        split = dataset_maps[dataset_id][\"split\"]\n",
    "        if os.path.exists(path):\n",
    "            print(\"Dataset already downloaded\")\n",
    "            dataset = load_from_disk(path)\n",
    "        else:\n",
    "            print(\"Dataset not downloaded yet\")\n",
    "            dataset = load_dataset(dataset_id, split=split)\n",
    "            dataset.save_to_disk(path)\n",
    "    else:\n",
    "        print(\"Dataset not found\")\n",
    "    return dataset\n",
    "\n",
    "### ds to pandas\n",
    "import pandas as pd\n",
    "\n",
    "# cur_dataset = \"zeqianli/HowToStep\"\n",
    "# cur_dataset = \"ajibawa-2023/WikiHow\"\n",
    "cur_dataset = \"ht-step\"\n",
    "ds = get_dataset(cur_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "\n",
    "from helpers.bert import bert_embedding\n",
    "\n",
    "def plot_dendrogram(model, dictionary, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "\n",
    "    def llf(id):\n",
    "        if id < n_samples:\n",
    "            return dictionary[id][\"variation_index\"]\n",
    "        else:\n",
    "            return '[%d %d %d]' % (linkage_matrix[id, 0], linkage_matrix[id, 1], linkage_matrix[id, 2])\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, leaf_label_func=llf, **kwargs)\n",
    "\n",
    "\n",
    "def cluster_similar_steps(variations):\n",
    "    \"\"\"\n",
    "    cluster similar steps together\n",
    "    \"\"\"\n",
    "\n",
    "    linearized = [step for variation in variations for step in variation]\n",
    "    print([step[\"variation_index\"] for step in linearized])\n",
    "\n",
    "    N = len(linearized)\n",
    "\n",
    "    embeddings = bert_embedding([step[\"headline\"] for step in linearized])\n",
    "\n",
    "    # sim_matrix = np.zeros((N, N))\n",
    "    # for i in range(N):\n",
    "    #     for j in range(i+1, N):\n",
    "    #         sim_matrix[i][j] = np.dot(embeddings[i], embeddings[j])\n",
    "    #         sim_matrix[j][i] = sim_matrix[i][j]\n",
    "    \n",
    "    clustering = AgglomerativeClustering(\n",
    "        n_clusters=None,\n",
    "        distance_threshold=0,\n",
    "        linkage='average',\n",
    "        metric='cosine'\n",
    "    ).fit(embeddings)\n",
    "\n",
    "    plot_dendrogram(clustering, linearized, truncate_mode='level', p=10)\n",
    "    \n",
    "    clusters = {}\n",
    "    for i, cluster_id in enumerate(clustering.labels_):\n",
    "        if cluster_id not in clusters:\n",
    "            clusters[cluster_id] = []\n",
    "        clusters[cluster_id].append(linearized[i])\n",
    "    return clusters\n",
    "\n",
    "variations = []\n",
    "task = ds[\"activity_info\"][0]\n",
    "\n",
    "print(task[\"task\"])\n",
    "\n",
    "for kk, vv in task[\"variations\"].items():\n",
    "    variations.append(vv)\n",
    "result = cluster_similar_steps(variations)\n",
    "\n",
    "for kk, vv in result.items():\n",
    "    print(\"Cluster\", kk)\n",
    "    for step in vv:\n",
    "        print(step[\"variation_index\"], step[\"headline\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from helpers.clip import ClipModel\n",
    "\n",
    "import json\n",
    "\n",
    "def get_sentence_frame_paths(video):\n",
    "    frames = []\n",
    "    for sentence in video[\"sentences\"]:\n",
    "        length = len(sentence[\"frame_paths\"])\n",
    "        if length == 0:\n",
    "            continue\n",
    "        frames.append(sentence[\"frame_paths\"][length//2])\n",
    "    return frames\n",
    "\n",
    "def get_all_frame_paths(video):\n",
    "    return [frame[\"path\"] for frame in video[\"frames\"].values()]\n",
    "\n",
    "def print_single_object(model, subgoal, key, obj):\n",
    "    print(f\"- {subgoal} - {key.capitalize()} = {obj['name']}\")\n",
    "    print(f\"\\t- {obj['description']}\")\n",
    "    print(f\"\\t- {obj['caption']}\")\n",
    "    img_path = obj[\"frame_paths\"][0]\n",
    "    if model:\n",
    "        img_paths = model.find_similar_per_text([\n",
    "            obj[\"caption\"],\n",
    "            obj[\"description\"],\n",
    "            \"random object\",\n",
    "        ])\n",
    "        img_path = img_paths[0]\n",
    "    ### Print image\n",
    "    print(img_path)\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "def print_all_objects(video, frame_paths):             \n",
    "    keys = ['materials', 'tools', 'outcomes']\n",
    "    print(f\"# Video: {video['video_id']}\")\n",
    "    model = None\n",
    "    if len(frame_paths) > 0:\n",
    "        model = ClipModel(frame_paths)\n",
    "    for summary in video['subgoal_summaries'].values():\n",
    "        for key in summary:\n",
    "            if key not in keys:\n",
    "                continue\n",
    "            for obj in summary[key]:\n",
    "                print_single_object(model, summary['title'], key, obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.bert import clustering_custom\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "task_id = \"carbonara\"  # Replace with your actual task_id\n",
    "video_file_path = f'static/results/{task_id}/video_data.json'\n",
    "subgoal_file_path = f'static/results/{task_id}/subgoal_data.json'\n",
    "\n",
    "with open(video_file_path, 'r') as file:\n",
    "    video_data = json.load(file)\n",
    "\n",
    "with open(subgoal_file_path, 'r') as file:\n",
    "    subgoal_data = json.load(file)\n",
    "\n",
    "def visualize_graph(G, ax):\n",
    "    node_colors = {\n",
    "        \"material\": \"skyblue\",\n",
    "        \"tool\": \"lightgreen\",\n",
    "        \"action\": \"lightcoral\",\n",
    "        \"outcome\": \"skyblue\"\n",
    "    }\n",
    "    pos = nx.nx_agraph.graphviz_layout(G, prog='dot', args='-Grankdir=TDshowboxes')\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=[node_colors[G.nodes[node][\"type\"]] for node in G.nodes], ax=ax)\n",
    "    ### add edge labels\n",
    "    edge_labels = nx.get_edge_attributes(G, 'label')\n",
    "    nx.draw_networkx_edges(G, pos, ax=ax)\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, ax=ax)\n",
    "    nx.draw_networkx_labels(G, pos, labels={node: node for node in G.nodes}, font_size=8, verticalalignment='baseline', ax=ax)\n",
    "    ### add separating horizontal lines between generations and add labels\n",
    "\n",
    "    unique_y = list(set([xy[1] for xy in pos.values()]))\n",
    "    unique_y = sorted(unique_y)\n",
    "    for i, y in enumerate(unique_y):\n",
    "        ax.axhline(y=y + 0.5, color='gray', linestyle='--', linewidth=0.5, zorder=0, alpha=0.5)\n",
    "    ax.set_axis_off()\n",
    "\n",
    "def build_and_visualize_graph(steps, ax):\n",
    "    \"\"\"\n",
    "    steps: list of dictionaries with keys \"action\", \"materials\", \"tools\", and \"outcomes\"\n",
    "    \"\"\"\n",
    "    nodes = {}\n",
    "    for i, step in enumerate(steps):\n",
    "        action = step[\"action\"] + \"_\" + str(i)\n",
    "        materials = step[\"materials\"]\n",
    "        tools = step[\"tools\"]\n",
    "        outcomes = step[\"outcomes\"]\n",
    "        for material in materials:\n",
    "            if material not in nodes:\n",
    "                nodes[material] = {\n",
    "                    \"edges\": [],\n",
    "                    \"type\": \"material\",\n",
    "                }\n",
    "            nodes[material][\"edges\"].append(action)\n",
    "        for tool in tools:\n",
    "            if tool not in nodes:\n",
    "                nodes[tool] = {\n",
    "                    \"edges\": [],\n",
    "                    \"type\": \"tool\",\n",
    "                }\n",
    "            nodes[tool][\"edges\"].append(action)\n",
    "        if action not in nodes:\n",
    "            nodes[action] = {\n",
    "                \"edges\": [],\n",
    "                \"type\": \"action\",\n",
    "            }\n",
    "        for outcome in outcomes:\n",
    "            if outcome not in nodes:\n",
    "                nodes[outcome] = {\n",
    "                    \"edges\": [],\n",
    "                    \"type\": \"outcome\",\n",
    "                }\n",
    "            nodes[action][\"edges\"].append(outcome)\n",
    "    ### Visualize directed graph the graph should flow to the bottom\n",
    "    G = nx.DiGraph()\n",
    "    for node, data in nodes.items():\n",
    "        G.add_node(node, name=node, type=data[\"type\"])\n",
    "        for edge in data[\"edges\"]:\n",
    "            G.add_edge(node, edge, label=\"\")\n",
    "\n",
    "    simplified_G = nx.DiGraph()\n",
    "    ### leave only the nodes that have type \"action\" and preserve all the edges\n",
    "    for node in G.nodes:\n",
    "        if G.nodes[node][\"type\"] != \"action\":\n",
    "            continue\n",
    "        simplified_G.add_node(node, name=node, type=G.nodes[node][\"type\"])\n",
    "    for edge in G.edges:\n",
    "        x, y = edge\n",
    "        if x not in simplified_G.nodes:\n",
    "            continue\n",
    "        if y in simplified_G.nodes:\n",
    "            continue\n",
    "        for other_edge in G.edges:\n",
    "            xx, yy = other_edge\n",
    "            if xx != y:\n",
    "                continue\n",
    "            if yy not in simplified_G.nodes:\n",
    "                continue\n",
    "            ### check if there is an edge from x to yy\n",
    "            if (x, yy) in simplified_G.edges:\n",
    "                continue\n",
    "            simplified_G.add_edge(x, yy, label = \"\")\n",
    "    G = simplified_G\n",
    "\n",
    "    ### remove nodes that have 1 incoming and 1 outgoing edge\n",
    "    while True:\n",
    "        to_remove = []\n",
    "        for node in G.nodes:\n",
    "            if len(list(G.predecessors(node))) == 1 and len(list(G.successors(node))) == 1:\n",
    "                to_remove.append(node)\n",
    "        if len(to_remove) == 0:\n",
    "            break\n",
    "        \n",
    "        for node in to_remove:\n",
    "            ### connect the predecessors and successors\n",
    "            pred = list(G.predecessors(node))[0]\n",
    "            pred_edge_label = G[pred][node][\"label\"]\n",
    "            succ = list(G.successors(node))[0]\n",
    "            succ_edge_label = G[node][succ][\"label\"]\n",
    "            new_label = pred_edge_label + \" \" + node + \" \" + succ_edge_label\n",
    "            G.add_edge(pred, succ, label=new_label)\n",
    "            G.remove_node(node)\n",
    "\n",
    "    visualize_graph(G, ax)\n",
    "\n",
    "def get_steps(all_steps, start, finish):\n",
    "    filter_out = [\"hands\", \"hand\"]\n",
    "    # filter_out = []\n",
    "    def filter_list(arr):\n",
    "        return [x for x in arr if x not in filter_out]\n",
    "\n",
    "    actions_set = []\n",
    "    steps = []\n",
    "    for step in all_steps[start:finish]:\n",
    "        materials = step[\"inputs\"]\n",
    "        # tools = step[\"tools\"]\n",
    "        outcomes = step[\"outcomes\"]\n",
    "        steps.append({\n",
    "            \"action\": step[\"action\"],\n",
    "            \"materials\": filter_list(materials),\n",
    "            \"tools\": [],\n",
    "            \"outcomes\": filter_list(outcomes),\n",
    "        })\n",
    "        is_new_action = True\n",
    "        for action in actions_set:\n",
    "            if action == step[\"action\"]:\n",
    "                is_new_action = False\n",
    "                break\n",
    "        if is_new_action:\n",
    "            actions_set.append(step[\"action\"])\n",
    "    return actions_set, steps\n",
    "\n",
    "\n",
    "def cluster_outcomes(tagged_outcomes, threshold=0.8):\n",
    "    outcomes = []\n",
    "    for tagged_outcome in tagged_outcomes:\n",
    "        outcomes.append(tagged_outcome[\"outcome\"])\n",
    "    labels = clustering_custom(outcomes, threshold)\n",
    "    #### show clusters\n",
    "    clusters = {}\n",
    "    for i, label in enumerate(labels):\n",
    "        if label not in clusters:\n",
    "            clusters[label] = []\n",
    "        clusters[label].append(tagged_outcomes[i])\n",
    "    return clusters\n",
    "\n",
    "### cluster outcomes\n",
    "# tagged_outcomes = []\n",
    "# for video in video_data:\n",
    "#     for step in video[\"steps\"]:\n",
    "#         for outcome in step[\"outcomes\"]:\n",
    "#             tagged_outcomes.append({\n",
    "#                 \"outcome\": outcome,\n",
    "#                 \"tag\": step[\"action\"],\n",
    "#                 \"video\": video[\"video_id\"]\n",
    "#             })\n",
    "\n",
    "# clusters = cluster_outcomes(tagged_outcomes)\n",
    "# clusters = sorted(clusters.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "# print(json.dumps(clusters, indent=4))\n",
    "\n",
    "### show the trees\n",
    "for video in video_data:\n",
    "\n",
    "    ### print full tree\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(25, 15))\n",
    "    actions_set, steps = get_steps(video[\"steps\"], 0, len(video[\"steps\"]))\n",
    "    build_and_visualize_graph(steps, axs)\n",
    "    actions_set, steps = get_steps(video[\"steps\"], 0, len(video[\"steps\"]))\n",
    "    \n",
    "    ### print subgoals\n",
    "    # fig, axs = plt.subplots(1, len(video[\"subgoals\"]), figsize=(5 * len(video[\"subgoals\"]), 10))\n",
    "    \n",
    "    # for index, subgoal in enumerate(video[\"subgoals\"]):\n",
    "    #     actions_set, steps = get_steps(video[\"steps\"], subgoal[\"start_index\"], subgoal[\"finish_index\"])\n",
    "    #     build_and_visualize_graph(steps, axs[index])\n",
    "    #     axs[index].set_title(subgoal[\"title\"])\n",
    "\n",
    "    #     # print(f\"## Subgoal {index + 1}: {subgoal['title']}\")\n",
    "    #     # print(f\"Description: {subgoal['description']}\")\n",
    "    #     # print(actions_set)\n",
    "    fig.suptitle(f\"{video['video_id']}\")\n",
    "    plt.show()\n",
    "    print()\n",
    "\n",
    "# info_per_subgoal = {}\n",
    "\n",
    "# for video in video_data:\n",
    "#     for subgoal in video[\"subgoals\"]:\n",
    "#         if subgoal[\"title\"] not in info_per_subgoal:\n",
    "#             info_per_subgoal[subgoal[\"title\"]] = []\n",
    "#         actions_set, steps = get_steps(video[\"steps\"], subgoal[\"start_index\"], subgoal[\"finish_index\"])\n",
    "#         info_per_subgoal[subgoal[\"title\"]].append({\n",
    "#             \"video_id\": video[\"video_id\"],\n",
    "#             \"description\": subgoal[\"description\"],\n",
    "#             \"steps\": steps,\n",
    "#             \"actions_set\": actions_set,\n",
    "#         })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "materials_per_subgoal = {}\n",
    "tools_per_subgoal = {}\n",
    "steps_per_subgoal = {}\n",
    "outcomes_per_subgoal = {}\n",
    "for video in video_data:\n",
    "    for id, summary in video['subgoal_summaries'].items():\n",
    "        id = summary['title']\n",
    "        materials = summary.get('materials', [])\n",
    "        tools = summary.get('tools', [])\n",
    "        steps = summary.get('steps', [])\n",
    "        outcomes = summary.get('outcomes', [])\n",
    "        if id not in materials_per_subgoal:\n",
    "            materials_per_subgoal[id] = []\n",
    "        if id not in tools_per_subgoal:\n",
    "            tools_per_subgoal[id] = []\n",
    "        if id not in steps_per_subgoal:\n",
    "            steps_per_subgoal[id] = []\n",
    "        if id not in outcomes_per_subgoal:\n",
    "            outcomes_per_subgoal[id] = []\n",
    "        for material in materials:\n",
    "            materials_per_subgoal[id].append(video[\"video_id\"] + \"-\" + material['name'])\n",
    "        for tool in tools:\n",
    "            tools_per_subgoal[id].append(video[\"video_id\"] + \"-\" + tool['name'])\n",
    "        for step in steps:\n",
    "            steps_per_subgoal[id].append(step[\"description\"])\n",
    "        for outcome in outcomes:\n",
    "            outcomes_per_subgoal[id].append(outcome[\"name\"])\n",
    "\n",
    "print(json.dumps(materials_per_subgoal, indent=4))\n",
    "\n",
    "print(json.dumps(tools_per_subgoal, indent=4))\n",
    "\n",
    "print(json.dumps(steps_per_subgoal, indent=4))\n",
    "\n",
    "print(json.dumps(outcomes_per_subgoal, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['materials', 'tools', 'outcomes']\n",
    "for video in video_data:\n",
    "    print(f\"# Video: {video['video_id']}\")\n",
    "    frame_paths = [frame[\"path\"] for frame in video[\"frames\"].values()]\n",
    "    model = ClipModel(frame_paths)\n",
    "    for subgoal, summary in video['subgoal_summaries'].items():\n",
    "        for key in summary:\n",
    "            if key not in keys:\n",
    "                continue\n",
    "            for obj in summary[key]:\n",
    "                print(f\"- {subgoal} - {key.capitalize()} - {obj['name']}\")\n",
    "                print(f\"\\t- {obj['description']}\")\n",
    "                print(f\"\\t- {obj['caption']}\")\n",
    "                # img_path = obj[\"frame_paths\"][0]\n",
    "                \n",
    "                img_paths = model.find_similar_per_text([\n",
    "                    obj['description'],\n",
    "                    obj['caption'],\n",
    "                    \"random object\",\n",
    "                ])\n",
    "                img_path = img_paths[0]\n",
    "                ### Print image\n",
    "                print(img_path)\n",
    "                img = cv2.imread(img_path)\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                plt.imshow(img)\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "                print()\n",
    "                print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIMILARITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for video in video_data:\n",
    "    print(\"##\", video['video_id'])\n",
    "    cur_steps = [*video['steps']]\n",
    "    cur_subgoals = [*video['subgoals']]\n",
    "\n",
    "    cur_subgoals = sorted(cur_subgoals, key=lambda x: x['start'])\n",
    "    print(\"### Subgoals:\")\n",
    "    for subgoal in cur_subgoals:\n",
    "        print(\"`\", subgoal[\"title\"], \"`\", end=\" --> \")\n",
    "    print()\n",
    "    for step in cur_steps:\n",
    "        print('-', step)\n",
    "        # for subgoal in cur_subgoals:\n",
    "        #     if step in subgoal['original_steps']:\n",
    "        #         print('  -', subgoal['title'])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_similarity_heatmap(steps_0, steps_1):\n",
    "    # Load pre-trained model\n",
    "    model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "    # Compute embeddings\n",
    "    embeddings_video_0 = model.encode(steps_0, convert_to_tensor=True).cpu()\n",
    "    embeddings_video_1 = model.encode(steps_1, convert_to_tensor=True).cpu()\n",
    "\n",
    "    # Compute cosine similarity matrix\n",
    "    cosine_sim_matrix = util.pytorch_cos_sim(embeddings_video_0, embeddings_video_1).numpy()\n",
    "\n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cosine_sim_matrix, annot=True, xticklabels=steps_1, yticklabels=steps_0, cmap='coolwarm')\n",
    "    # Tilt x-axis labels\n",
    "    plt.xticks(rotation=-45, ha='left')\n",
    "\n",
    "    plt.xlabel('Steps Video 1')\n",
    "    plt.ylabel('Steps Video 0')\n",
    "    plt.title('Semantic Similarity Heatmap')\n",
    "    plt.show()\n",
    "\n",
    "def draw_chain_heatmap(step_sets):\n",
    "    chain_count = {}\n",
    "    nexts_count = {}\n",
    "    for steps in step_sets:\n",
    "        prev = 'start'\n",
    "        for step in [*steps, 'end']:\n",
    "            if prev not in chain_count:\n",
    "                chain_count[prev] = {}\n",
    "            if step not in chain_count[prev]:\n",
    "                chain_count[prev][step] = 0\n",
    "            chain_count[prev][step] += 1\n",
    "\n",
    "            if prev not in nexts_count:\n",
    "                nexts_count[prev] = 0\n",
    "            nexts_count[prev] += 1\n",
    "            prev = step\n",
    "    \n",
    "    keys = [*chain_count.keys(), 'end']\n",
    "\n",
    "    chain_matrix = np.zeros((len(keys), len(keys)))\n",
    "    for i, key in enumerate(keys):\n",
    "        for j, key2 in enumerate(keys):\n",
    "            if key in chain_count and key2 in chain_count[key]:\n",
    "                chain_matrix[i, j] = chain_count[key][key2] / nexts_count[key]\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(chain_matrix, annot=True, xticklabels=keys, yticklabels=keys, cmap='coolwarm')\n",
    "    # Tilt x-axis labels\n",
    "    plt.xticks(rotation=-45, ha='left')\n",
    "\n",
    "    plt.xlabel('2')\n",
    "    plt.ylabel('1')\n",
    "    plt.title('Chain Heatmap')\n",
    "    \n",
    "\n",
    "combines = {\n",
    "    # 'Prepare Meat': 'Prepare and Cook Meat',\n",
    "    # 'Cook Meat': 'Prepare and Cook Meat',\n",
    "    # 'Mix Egg and Cheese': 'Combine Ingredients',\n",
    "    # 'Combine Pasta and Meat': 'Combine Ingredients',\n",
    "    # 'Add Egg Mixture': 'Combine Ingredients',\n",
    "}\n",
    "\n",
    "step_sets = []\n",
    "for video in video_data:\n",
    "    cur_subgoals = [*video['subgoals']]\n",
    "    cur_subgoals = sorted(cur_subgoals, key=lambda x: x['start'])\n",
    "    steps = []\n",
    "    for subgoal in cur_subgoals:\n",
    "        if subgoal[\"title\"] != '':\n",
    "            if subgoal[\"title\"] in combines:\n",
    "                steps.append(combines[subgoal[\"title\"]])\n",
    "            else:\n",
    "                steps.append(subgoal[\"title\"])\n",
    "    combined_steps = []\n",
    "    for step in steps:\n",
    "        if len(combined_steps) > 0 and combined_steps[-1] == step:\n",
    "            continue\n",
    "        combined_steps.append(step)\n",
    "    step_sets.append(combined_steps)\n",
    "\n",
    "draw_chain_heatmap(step_sets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "alignment_file_path = f'static/results/{task_id}/alignment_sets.json'\n",
    "\n",
    "with open(alignment_file_path, 'r') as file:\n",
    "    alignment_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_comparisons = {}\n",
    "for approach, alignment_sets in alignment_data.items():\n",
    "    for alignment_set in alignment_sets:\n",
    "        video_id = alignment_set['video_id']\n",
    "        alignments = alignment_set['alignments']\n",
    "        for alignment in alignments:\n",
    "            other_video_id = alignment['other_video_id']\n",
    "            key = video_id + '-' + other_video_id\n",
    "            if approach.endswith('baseline_1'):\n",
    "                key = other_video_id + '-' + video_id\n",
    "            if key not in pair_comparisons:\n",
    "                pair_comparisons[key] = {}\n",
    "            if approach not in pair_comparisons[key]:\n",
    "                pair_comparisons[key][approach] = []\n",
    "            pair_comparisons[key][approach].append(alignment)\n",
    "\n",
    "video_id_to_index = {}\n",
    "for pair, comparisons in pair_comparisons.items():\n",
    "    video_0, video_1 = pair.split('-')\n",
    "    if video_0 not in video_id_to_index:\n",
    "        video_id_to_index[video_0] = len(video_id_to_index)\n",
    "    if video_1 not in video_id_to_index:\n",
    "        video_id_to_index[video_1] = len(video_id_to_index)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def combine_embed_plot_alignments(video_id, approach):\n",
    "    # Collect all alignments for the given video_id\n",
    "    alignments = []\n",
    "    for pair, comparisons in pair_comparisons.items():\n",
    "        video_0, video_1 = pair.split('-')\n",
    "        if video_0 != video_id:\n",
    "            continue\n",
    "        for a, alignment_list in comparisons.items():\n",
    "            if a != approach:\n",
    "                continue \n",
    "            for alignment in alignment_list:\n",
    "                alignments.append({\n",
    "                    'text': alignment['alignment_description'],\n",
    "                    'subgoal': alignment['subgoal_title'],\n",
    "                    'relation': alignment['relation'],\n",
    "                    'aspect': alignment['aspect'],\n",
    "                    'video_id': video_1,\n",
    "                })\n",
    "    \n",
    "    # Load pre-trained model\n",
    "    model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "    \n",
    "    # Compute embeddings\n",
    "    embeddings = model.encode([a['text'] for a in alignments], convert_to_tensor=True).cpu().numpy()\n",
    "\n",
    "    similarity_matrix = util.pytorch_cos_sim(embeddings, embeddings).numpy()\n",
    "    ### color alignments that have similarity > THRESHOLD into the same color\n",
    "    THRESHOLD = 0.8\n",
    "    \n",
    "    colors = [0 for i in range(len(alignments))]\n",
    "    n_colors = 0\n",
    "    for i in range(len(alignments)):\n",
    "        if colors[i] == 0:\n",
    "            n_colors += 1\n",
    "            colors[i] = n_colors\n",
    "        for j in range(i+1, len(alignments)):\n",
    "            if similarity_matrix[i, j] > THRESHOLD:\n",
    "                colors[j] = colors[i]\n",
    "    same_color = {}\n",
    "    for i, a in enumerate(alignments):\n",
    "        a['color'] = colors[i] - 1\n",
    "        if a['color'] not in same_color:\n",
    "            same_color[a['color']] = []\n",
    "        same_color[a['color']].append(a)\n",
    "    palette = sns.color_palette(\"hsv\", n_colors)\n",
    "    print(n_colors)\n",
    "    for color, color_alignments in same_color.items():\n",
    "        n_alternative = len([a for a in color_alignments if a['relation'] == 'alternative'])\n",
    "        n_additional = len([a for a in color_alignments if a['relation'] == 'additional'])\n",
    "        subgoals = set([a['subgoal'] for a in color_alignments])\n",
    "        aspects = set([a['aspect'] for a in color_alignments])\n",
    "        video_ids = set([a['video_id'] for a in color_alignments])\n",
    "        print(color, \"Total -\", len(color_alignments),)\n",
    "        print(\"Alternative -\", n_alternative, \"Additional -\", n_additional)\n",
    "        print(\"Subgoals -\", subgoals)\n",
    "        print(\"Aspects -\", aspects)\n",
    "        print(\"Video IDs -\", video_ids)\n",
    "        print(\"Descriptions:\\n\", \"\\t\\n\".join([a['text'] for a in color_alignments]))\n",
    "        print()\n",
    "        print()\n",
    "    \n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_result = pca.fit_transform(embeddings)\n",
    "    \n",
    "    # Plot PCA result\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(pca_result[:, 0], pca_result[:, 1])\n",
    "    for i, a in enumerate(alignments):\n",
    "        color = palette[a['color']]\n",
    "        # label = f\"{a['subgoal']} ({a['relation']}-{a['aspect']})\"\n",
    "        label = f\"{a['video_id']}\"\n",
    "        plt.annotate(label, (pca_result[i, 0], pca_result[i, 1]), color=color)\n",
    "    plt.xlabel('PCA 1')\n",
    "    plt.ylabel('PCA 2')\n",
    "    plt.title(f'PCA of Alignments for Video {video_id} ({approach})')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "video_id = [*video_id_to_index.keys()][0]\n",
    "print(video_id)\n",
    "pca_result = combine_embed_plot_alignments(video_id, 'approach_1')\n",
    "print(pca_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INPUT/METHOD/OUTPUT distance calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distances(aspect_groups):\n",
    "    distance_matrix_per_subgoal_aspect = {}\n",
    "\n",
    "    approach = 'approach_1'\n",
    "    N = len(video_id_to_index)\n",
    "\n",
    "    for pair, comparisons in pair_comparisons.items():\n",
    "        video_0, video_1 = pair.split('-')\n",
    "        index_0 = video_id_to_index[video_0]\n",
    "        index_1 = video_id_to_index[video_1]\n",
    "\n",
    "        for alignment in comparisons[approach]:\n",
    "            subgoal = alignment['subgoal_title']\n",
    "            aspect = alignment['aspect']\n",
    "            relation = alignment['relation']\n",
    "            if subgoal not in distance_matrix_per_subgoal_aspect:\n",
    "                distance_matrix_per_subgoal_aspect[subgoal] = {}\n",
    "            if aspect not in distance_matrix_per_subgoal_aspect[subgoal]:\n",
    "                distance_matrix_per_subgoal_aspect[subgoal][aspect] = np.zeros((N, N))\n",
    "            if relation == 'alternative':\n",
    "                distance_matrix_per_subgoal_aspect[subgoal][aspect][index_0, index_1] += (alignment['importance'] - 1) / 4\n",
    "                # distance_matrix_per_subgoal_aspect[subgoal][aspect][index_0, index_1] += 1\n",
    "\n",
    "    def aggregate_distance_matrix(matrices, keys):\n",
    "        result = np.zeros((N, N))\n",
    "        found = False\n",
    "        for key, matrix in matrices.items():\n",
    "            if matrix is None:\n",
    "                continue\n",
    "            if key in keys:\n",
    "                result += matrix\n",
    "                found = True\n",
    "        if not found:\n",
    "            return None\n",
    "        result /= len(keys)\n",
    "        return result\n",
    "\n",
    "    distance_matrix_per_subgoal = {}\n",
    "    for subgoal, distance_matrix_per_aspect in distance_matrix_per_subgoal_aspect.items():\n",
    "        result = np.zeros((N, N))\n",
    "        for aspect_group in aspect_groups:\n",
    "            cur_matrix = aggregate_distance_matrix(distance_matrix_per_aspect, aspect_group)\n",
    "            if cur_matrix is None:\n",
    "                continue\n",
    "            result += cur_matrix\n",
    "        distance_matrix_per_subgoal[subgoal] = result / len(aspect_groups)\n",
    "        ## Normalize\n",
    "        max_value = np.max(distance_matrix_per_subgoal[subgoal])\n",
    "        if max_value > 0:\n",
    "            distance_matrix_per_subgoal[subgoal] = distance_matrix_per_subgoal[subgoal] / max_value\n",
    "\n",
    "    all_subgoals = [*distance_matrix_per_subgoal.keys()]\n",
    "    distance_marix = aggregate_distance_matrix(distance_matrix_per_subgoal, all_subgoals)\n",
    "\n",
    "    distance_matrices = {\n",
    "        'video-level': distance_marix,\n",
    "    }\n",
    "\n",
    "    ## draw distance heatmap for each subgoal\n",
    "    for subgoal, matrix in distance_matrix_per_subgoal.items():\n",
    "        distance_matrices[subgoal] = matrix\n",
    "    \n",
    "    return distance_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def draw_heatmaps_of_matrices(distance_matrices, labels):\n",
    "    ## draw multiple distance matrices in one figure make sure to square out the figure as much as possible\n",
    "    fig, axs = plt.subplots(1, len(distance_matrices), figsize=(10 * len(distance_matrices), 8))\n",
    "\n",
    "    for idx, (title, matrix) in enumerate(distance_matrices.items()):\n",
    "        cur_ax = axs[idx]\n",
    "        sns.heatmap(\n",
    "            matrix,\n",
    "            annot=True,\n",
    "            ax=cur_ax,\n",
    "            cmap='coolwarm',\n",
    "            xticklabels=labels,\n",
    "            yticklabels=labels,\n",
    "            cbar=True,\n",
    "        )\n",
    "        cur_ax.set_title(\"Heatmap: \" + title)\n",
    "        cur_ax.set_xlabel('Other Video')\n",
    "        cur_ax.set_ylabel('Current Video')\n",
    "    plt.show()\n",
    "\n",
    "groups = {\n",
    "    'input': [\n",
    "        ['materials'],\n",
    "        # ['instructions', 'tools'], ### skip 'warnings', 'explanations', 'tips'\n",
    "        # ['outcomes']\n",
    "    ],\n",
    "    'method': [\n",
    "        # ['materials'],\n",
    "        ['instructions', 'tools'], ### skip 'warnings', 'explanations', 'tips'\n",
    "        # ['outcomes']\n",
    "    ],\n",
    "    'output': [\n",
    "        # ['materials'],\n",
    "        # ['instructions', 'tools'], ### skip 'warnings', 'explanations', 'tips'\n",
    "        ['outcomes']\n",
    "    ],\n",
    "    \"overall\": [\n",
    "        ['materials'],\n",
    "        ['instructions', 'tools'], ### skip 'warnings', 'explanations', 'tips'\n",
    "        ['outcomes']\n",
    "    ],\n",
    "}\n",
    "\n",
    "distances_per_group = {}\n",
    "\n",
    "for group in groups:\n",
    "    distances_per_group[group] = get_distances(groups[group])\n",
    "\n",
    "draw_heatmaps_of_matrices(distances_per_group['input'], [*video_id_to_index.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_plot_2D(ax, points, labels):\n",
    "    \n",
    "    ### Plot the coordinates\n",
    "    ax.scatter(points[:, 0], points[:, 1], s=100)\n",
    "    for i, label in enumerate(labels):\n",
    "        ax.text(points[i, 0], points[i, 1], label, fontsize=12)\n",
    "    ax.set_title('MDS Visualization of Distance Matrix')\n",
    "\n",
    "def matrix_to_points(matrix, dims):\n",
    "    sym_matrix = (matrix + matrix.T) / 2\n",
    "\n",
    "    ### Transform distance matrix to coordinates\n",
    "    from sklearn.manifold import MDS\n",
    "\n",
    "    embedding = MDS(n_components=dims, dissimilarity='precomputed', random_state=42)\n",
    "    X_transformed = embedding.fit_transform(sym_matrix)\n",
    "    print(embedding.stress_)\n",
    "    \n",
    "    if dims == 1:\n",
    "        ## add a second dimension and set it to 0\n",
    "        X_transformed = np.concatenate([np.zeros((X_transformed.shape[0], 1)), X_transformed], axis=1)\n",
    "    return X_transformed\n",
    "\n",
    "fig, axs = plt.subplots(1, len(distances_per_group), figsize=(10 * len(distances_per_group), 8))\n",
    "for idx, (group, distance_matrices) in enumerate(distances_per_group.items()):\n",
    "    labels = [*video_id_to_index.keys()]\n",
    "    points = matrix_to_points(distance_matrices['video-level'], 1)\n",
    "    scatter_plot_2D(axs[idx], points, labels)\n",
    "    axs[idx].set_title(f'{group}')\n",
    "    axs[idx].set_xlabel('Dimension 1')\n",
    "    axs[idx].set_ylabel('Dimension 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Output alignments from each approach side-by-side for each comparison\n",
    "\"\"\"\n",
    "for key, comparisons in pair_comparisons.items():\n",
    "    print(\"##\", key)\n",
    "    print()\n",
    "    approaches = [*comparisons.keys()]\n",
    "    print(\"|\", end=\"\")\n",
    "    for approach in approaches:\n",
    "        print(approach, \"|\", end=\"\")\n",
    "    print()\n",
    "    print(\"|\", end=\"\")\n",
    "    for approach in approaches:\n",
    "        print(\"---|\", end=\"\")\n",
    "    print()\n",
    "    max_alignments = max([len(comparisons[approach]) for approach in approaches if approach in comparisons])\n",
    "    for i in range(max_alignments):\n",
    "        print(\"|\", end=\"\")\n",
    "        for approach in approaches:\n",
    "            if approach in comparisons:\n",
    "                if i < len(comparisons[approach]):\n",
    "                    alignment = comparisons[approach][i]\n",
    "                    alignment_str = \"- ({subgoal_title}) **{relation}** **{aspect}**: {alignment_description}\".format(**alignment)\n",
    "                    print(alignment_str, \"|\", end=\"\")\n",
    "                else:\n",
    "                    print(\" |\", end=\"\")\n",
    "            else:\n",
    "                print(\" |\", end=\"\")\n",
    "        print()\n",
    "    print()\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "starlab-video-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
