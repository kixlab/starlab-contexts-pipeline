{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAMEWORK_PATH = \"./static/results/framework/\"\n",
    "MIN_VIDEOS = 20\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "from preprocess import pre_process_videos\n",
    "\n",
    "def get_muffin_video_transcripts():\n",
    "    library_metadata = {}\n",
    "    with open(\"./metadata.json\") as f:\n",
    "        library_metadata = json.load(f)\n",
    "\n",
    "    task_metadata = library_metadata[\"muffins\"]\n",
    "\n",
    "    muffin_videos = pre_process_videos(task_metadata[\"videos\"])\n",
    "    \n",
    "    transcripts = []\n",
    "    for video in muffin_videos:\n",
    "        url = f\"https://www.youtube.com/watch?v={video.video_id}\"\n",
    "        title = video.metadata[\"title\"]\n",
    "        content = \"\"\n",
    "        transcript = []\n",
    "        for sentence in video.sentences:\n",
    "            if sentence['text'].strip() == \"\":\n",
    "                continue\n",
    "            content += f\"{sentence['text']}\\n\"\n",
    "            transcript.append({\n",
    "                \"text\": sentence['text'],\n",
    "                \"start\": sentence['start'],\n",
    "                \"end\": sentence['finish'],\n",
    "            })\n",
    "\n",
    "        transcripts.append({\n",
    "            \"url\": url,\n",
    "            \"title\": \"Making Muffins\",\n",
    "            \"original_title\": title,\n",
    "            \"content\": content,\n",
    "            \"transcript\": transcript,\n",
    "        })\n",
    "    return transcripts\n",
    "\n",
    "def get_muffin_articles():\n",
    "    database_path = \"./static/datasets/muffin_articles/\"\n",
    "    articles = []\n",
    "    \n",
    "    for filename in os.listdir(database_path):\n",
    "        with open(database_path + filename) as f:\n",
    "            ### read line-by-line\n",
    "            url = f.readline()\n",
    "            title = f.readline()\n",
    "            content = \"\"\n",
    "            transcript = []\n",
    "            for idx, line in enumerate(f):\n",
    "                if line.strip() == \"\":\n",
    "                    continue\n",
    "                content += line\n",
    "                transcript.append({\n",
    "                    \"text\": line.strip(),\n",
    "                    \"start\": idx,\n",
    "                    \"end\": idx + 1,\n",
    "                })\n",
    "\n",
    "            articles.append({\n",
    "                \"url\": url,\n",
    "                \"original_title\": title,\n",
    "                \"title\": \"Making Muffins\",\n",
    "                \"content\": content,\n",
    "                \"transcript\": transcript,\n",
    "            })\n",
    "    return articles\n",
    "\n",
    "def get_dataset_muffins(task, dummy=\"\"):\n",
    "    dataset_filepath = f\"{FRAMEWORK_PATH}{task.replace(' ', '_').lower()}_{dummy}.json\"\n",
    "    if os.path.exists(dataset_filepath):\n",
    "        with open(dataset_filepath) as f:\n",
    "            dataset = json.load(f)\n",
    "        return dataset\n",
    "\n",
    "    dataset = get_muffin_articles()\n",
    "    dataset = dataset + get_muffin_video_transcripts()\n",
    "    print(f\"Number of articles: {len(dataset)}\")\n",
    "\n",
    "    # dataset = add_info_labels_to_dataset(dataset, task)\n",
    "\n",
    "    with open(dataset_filepath, \"w\") as f:\n",
    "        json.dump(dataset, f, indent=4)\n",
    "    return dataset\n",
    "\n",
    "### Handle CrossTask data;\n",
    "import csv\n",
    "from helpers.video_scripts import extract_transcript\n",
    "\n",
    "def library_cross_task():\n",
    "    library = []\n",
    "    PATH = \"./static/datasets/crosstask/\"\n",
    "    library_path = os.path.join(PATH, \"library.json\")\n",
    "    \n",
    "    if os.path.exists(library_path):\n",
    "        with open(library_path, \"r\") as f:\n",
    "            library = json.load(f)\n",
    "            return library\n",
    "\n",
    "    tasks_path = os.path.join(PATH, \"crosstask_release/tasks_primary.txt\")\n",
    "    videos_path = os.path.join(PATH, \"crosstask_release/videos.csv\")\n",
    "    videos_val_path = os.path.join(PATH, \"crosstask_release/videos_val.csv\")\n",
    "\n",
    "    \"\"\"\n",
    "    Task ID\n",
    "    Task name\n",
    "    URL of corresponding WikiHow page\n",
    "    Number of steps\n",
    "    Ordered list of comma-separated steps of the task\n",
    "    \"\"\"\n",
    "    task_obj_ids = [\"task_id\", \"task_name\", \"url\", \"num_steps\", \"steps\"]\n",
    "\n",
    "    with open(tasks_path) as f:\n",
    "        lines = f.readlines()\n",
    "        for start_idx in range(0, len(lines), 6):\n",
    "            cur_task = {}\n",
    "            finished = False\n",
    "            for idx, task_obj_id in enumerate(task_obj_ids):\n",
    "                if start_idx + idx >= len(lines):\n",
    "                    finished = True\n",
    "                    break\n",
    "                cur_task[task_obj_id] = lines[start_idx + idx].strip()\n",
    "            if finished is False:\n",
    "                library.append(cur_task)\n",
    "\n",
    "    for task in library:\n",
    "        task[\"steps\"] = task[\"steps\"].split(\",\")\n",
    "        task[\"videos\"] = []\n",
    "\n",
    "    for videos_path in [videos_path, videos_val_path]:\n",
    "        with open(videos_path) as f:\n",
    "            reader = csv.reader(f)\n",
    "            for row in reader:\n",
    "                task_id = row[0]\n",
    "                video_id = row[1]\n",
    "                video_url = row[2]\n",
    "                for task in library:\n",
    "                    if task[\"task_id\"] == task_id:\n",
    "                        task[\"videos\"].append({\n",
    "                            \"video_id\": video_id,\n",
    "                            \"video_url\": video_url,\n",
    "                        })\n",
    "\n",
    "    def get_language(video_subtitles_path):\n",
    "        with open(video_subtitles_path) as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                if \"Language:\" in line:\n",
    "                    return line.split(\":\")[1].strip()\n",
    "        return None\n",
    "\n",
    "\n",
    "    SUBTITLES_PATH = os.path.join(PATH, \"subtitles\")\n",
    "    for task in library:\n",
    "        for video in task[\"videos\"]:\n",
    "            video_id = video[\"video_id\"]\n",
    "            video_subtitles_path = os.path.join(SUBTITLES_PATH, f\"{video_id}.vtt\")\n",
    "            video[\"subtitles\"] = []\n",
    "\n",
    "            language = get_language(video_subtitles_path)\n",
    "            if language == \"en\":\n",
    "                video[\"subtitles\"] = extract_transcript(video_subtitles_path, None)\n",
    "\n",
    "    ANNOTATIONS_PATH = os.path.join(PATH, \"crosstask_release/annotations/\")\n",
    "\n",
    "    for task in library:\n",
    "        for video in task[\"videos\"]:\n",
    "            video[\"annotations\"] = []\n",
    "            annotation_path = os.path.join(ANNOTATIONS_PATH, f\"{task['task_id']}_{video['video_id']}.csv\")\n",
    "            if os.path.exists(annotation_path):\n",
    "                with open(annotation_path) as f:\n",
    "                    reader = csv.reader(f)\n",
    "                    for row in reader:\n",
    "                        video[\"annotations\"].append({\n",
    "                            \"step\": float(row[0]),\n",
    "                            \"start\": float(row[1]),\n",
    "                            \"end\": float(row[2]),\n",
    "                        })\n",
    "            else:\n",
    "                print(f\"No annotation found for {task['task_id']}_{video['video_id']}\")\n",
    "\n",
    "    ### label subtitles with step\n",
    "    for task in library:\n",
    "        for video in task[\"videos\"]:\n",
    "            annotated_subtitles = []\n",
    "            for subtitle in video[\"subtitles\"]:\n",
    "                cur_step = None\n",
    "                for annotation in video[\"annotations\"]:\n",
    "                    if subtitle[\"start\"] >= annotation[\"start\"] and subtitle[\"finish\"] <= annotation[\"end\"]:\n",
    "                        cur_step = task[\"steps\"][int(annotation[\"step\"]) - 1]\n",
    "                        break\n",
    "                annotated_subtitles.append({\n",
    "                    **subtitle,\n",
    "                    \"step\": cur_step,\n",
    "                })\n",
    "            video[\"subtitles\"] = annotated_subtitles\n",
    "\n",
    "    ### restructure to be similar to the `dataset`\n",
    "\n",
    "    ### save library as json\n",
    "    with open(library_path, \"w\") as f:\n",
    "        json.dump(library, f, indent=4)\n",
    "\n",
    "\n",
    "def get_dataset_cross_task(task):\n",
    "    \"\"\"\n",
    "    return dataset with the given task with a structure similar to the `dataset`\n",
    "    \"\"\"\n",
    "    library = library_cross_task()\n",
    "    dataset = []\n",
    "    for _task in library:\n",
    "        if _task[\"task_name\"] == task:\n",
    "            for video in _task[\"videos\"]:\n",
    "                content = \"\"\n",
    "                transcript = []\n",
    "                for subtitle in video[\"subtitles\"]:\n",
    "                    content += f\"{subtitle['text']} \"\n",
    "                    transcript.append({\n",
    "                        \"text\": subtitle['text'],\n",
    "                        \"start\": subtitle['start'],\n",
    "                        \"end\": subtitle['finish'],\n",
    "                    })\n",
    "                dataset.append({\n",
    "                    \"id\": video[\"video_id\"],\n",
    "                    \"url\": video[\"video_url\"],\n",
    "                    \"title\": task,\n",
    "                    \"original_title\": video[\"task\"],\n",
    "                    \"content\": content,\n",
    "                    \"transcript\": transcript,\n",
    "                    \"steps\": [],\n",
    "                    \"ipo\": [],\n",
    "                    \"processed_ipos\": [],\n",
    "                })\n",
    "\n",
    "    ### check if content is enough\n",
    "    filtered_dataset = []\n",
    "    for article in dataset:\n",
    "        if len(article[\"content\"]) < 100:\n",
    "            continue\n",
    "        filtered_dataset.append(article)\n",
    "    dataset = filtered_dataset\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def preprocess_cross_task(task, dummy=\"\"):\n",
    "    dataset_filepath = f\"{FRAMEWORK_PATH}{task.replace(' ', '_').lower()}_{dummy}.json\"\n",
    "    if os.path.exists(dataset_filepath):\n",
    "        with open(dataset_filepath) as f:\n",
    "            dataset = json.load(f)\n",
    "        return dataset\n",
    "\n",
    "\n",
    "    dataset = get_dataset_cross_task(task)\n",
    "    print(f\"Dataset for {task}: {len(dataset)}\")\n",
    "\n",
    "    with open(dataset_filepath, \"w\") as f:\n",
    "        json.dump(dataset, f, indent=4)\n",
    "    return dataset\n",
    "\n",
    "def preprocess_custom_dataset(task, dummy=\"\"):\n",
    "    dataset_filepath = f\"{FRAMEWORK_PATH}{task.replace(' ', '_').lower()}_{dummy}.json\"\n",
    "    if os.path.exists(dataset_filepath):\n",
    "        with open(dataset_filepath) as f:\n",
    "            dataset = json.load(f)\n",
    "        return dataset\n",
    "    \n",
    "    custom_tasks_path = \"./static/datasets/custom-dataset/videos_tasks_per_category.json\"\n",
    "    \n",
    "    with open(custom_tasks_path) as f:\n",
    "        custom_tasks = json.load(f)\n",
    "\n",
    "    videos = []\n",
    "    category = None\n",
    "    \n",
    "    for _category in custom_tasks:\n",
    "    \n",
    "        for task_info in custom_tasks[_category]:\n",
    "            if len(task_info[\"videos\"]) < MIN_VIDEOS:\n",
    "                continue\n",
    "            _task = task_info[\"task_details\"][\"title\"]\n",
    "    \n",
    "            if _task == task:\n",
    "                videos.extend(task_info[\"videos\"])\n",
    "                category = _category\n",
    "    \n",
    "\n",
    "    if category is None:\n",
    "        raise ValueError(f\"Task {task} not found in any category\")\n",
    "\n",
    "    dataset = []\n",
    "    for video in videos:\n",
    "        content = \"\"\n",
    "        for subtitle in video[\"transcript\"]:\n",
    "            content += f\"{subtitle['text']} \"\n",
    "        dataset.append({\n",
    "            \"id\": video[\"id\"],\n",
    "            \"url\": \"https://www.youtube.com/watch?v=\" + video[\"id\"],\n",
    "            \"title\": task,\n",
    "            \"original_title\": video[\"title\"],\n",
    "            \"category\": category,\n",
    "            \"content\": content,\n",
    "            \"transcript\": video[\"transcript\"],\n",
    "            \"steps\": [],\n",
    "            \"ipo\": [],\n",
    "            \"processed_ipos\": [],\n",
    "        })\n",
    "\n",
    "    ### check if content is enough\n",
    "    filtered_dataset = []\n",
    "    for article in dataset:\n",
    "        if len(article[\"content\"]) < 100:\n",
    "            continue\n",
    "        filtered_dataset.append(article)\n",
    "    dataset = filtered_dataset\n",
    "\n",
    "    print(f\"Dataset for {task}: {len(dataset)}\")\n",
    "\n",
    "    with open(dataset_filepath, \"w\") as f:\n",
    "        json.dump(dataset, f, indent=4)\n",
    "    return dataset    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from prompts.stupid_experiment_3 import segment_transcript_stupid\n",
    "\n",
    "\n",
    "\n",
    "def segment_videos(task, dataset, dummy = \"\"):\n",
    "    path = os.path.join(FRAMEWORK_PATH, f'{task.replace(\" \", \"_\").lower()}_segmentation_{dummy}.json')\n",
    "    if os.path.exists(path):\n",
    "        with open(path) as f:\n",
    "            return json.load(f)\n",
    "        \n",
    "    for video in dataset:\n",
    "        video['segmentation'] = segment_transcript_stupid(video['content'], TAXONOMY)\n",
    "\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(dataset, f, indent=4)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describing Scope (attempt 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.bert import bert_embedding, clustering_custom\n",
    "from prompts.stupid_experiment_3 import form_information_units_few_shot\n",
    "\n",
    "def build_information_units_v0(dataset, taskname, information_unit_similarity_threshold=0.8):\n",
    "    parent_path = os.path.join(FRAMEWORK_PATH, f'{taskname}')\n",
    "    if not os.path.exists(parent_path):\n",
    "        os.makedirs(parent_path)\n",
    "\n",
    "    path = os.path.join(parent_path, \"information_units_v0.json\")\n",
    "    if os.path.exists(path):\n",
    "        with open(path) as f:\n",
    "            all_data = json.load(f)\n",
    "            dataset = all_data[\"dataset\"]\n",
    "            information_units = all_data[\"information_units\"]\n",
    "            return dataset, information_units\n",
    "    \n",
    "    for video_idx, video in enumerate(dataset):\n",
    "        if \"pieces\" in video:\n",
    "            pieces = video[\"pieces\"]\n",
    "        else:\n",
    "            ### forming the information units (conceptually should be easily redefinable)\n",
    "            pieces = form_information_units_few_shot(video['title'], video['transcript'])\n",
    "            video['pieces'] = []\n",
    "            for i, piece in enumerate(pieces):\n",
    "                video['pieces'].append({\n",
    "                    \"piece_id\": f\"{video_idx}_{i}\",\n",
    "                    \"url\": video['url'],\n",
    "                    **piece,\n",
    "                })\n",
    "\n",
    "    all_pieces = []\n",
    "    for video in dataset:\n",
    "        for piece in video['pieces']:\n",
    "            all_pieces.append(piece)\n",
    "\n",
    "    #### cluster similar pieces in `all_pieces`\n",
    "    information_units = {}\n",
    "\n",
    "    unit_labels = clustering_custom([piece[\"content\"] for piece in all_pieces], information_unit_similarity_threshold)\n",
    "    for i, piece in enumerate(all_pieces):\n",
    "        cur_unit_id = f\"unit_{unit_labels[i]}\"\n",
    "        piece[\"unit_id\"] = cur_unit_id\n",
    "        if cur_unit_id not in information_units:\n",
    "            ### first piece is the representative of the cluster (IU)\n",
    "            information_units[cur_unit_id] = {\n",
    "                \"content\": piece[\"content\"],\n",
    "                \"content_type\": piece[\"content_type\"],\n",
    "                \"instances\": [piece[\"piece_id\"]],\n",
    "            }\n",
    "        else:\n",
    "            information_units[cur_unit_id][\"instances\"].append(piece[\"piece_id\"])\n",
    "\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump({\"dataset\": dataset, \"information_units\": information_units}, f, indent=4)\n",
    "\n",
    "    return dataset, information_units\n",
    "\n",
    "def build_codebook_v0(dataset, taskname, context_schema):\n",
    "    \"\"\"\n",
    "    analyze each video and extract candidate labels for the schema\n",
    "    cluster candidate labels to form codebook\n",
    "    return codebook\n",
    "    \"\"\"\n",
    "    path = os.path.join(FRAMEWORK_PATH, f'{taskname}', \"codebook_v0.json\")\n",
    "    if os.path.exists(path):\n",
    "        with open(path) as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    ### TODO: code the `what_is_missing` parts to recognize candidate schemas and update the `context_schema`\n",
    "\n",
    "    codebook = {}\n",
    "    return codebook\n",
    "\n",
    "def label_according_to_codebook_v0(dataset, codebook):\n",
    "    \"\"\"\n",
    "    label information pieces in each video according to the codebook and return `piece-label` dictionary\n",
    "    \"\"\"\n",
    "    cur_cim = {}\n",
    "    return cur_cim\n",
    "\n",
    "def critic_cim_v0(dataset, information_units, codebook, cim):\n",
    "    \"\"\"\n",
    "    sample X labeled pieces and run a critic LLM to check if the context labels are sufficient. Provide the full video context to the critic LLM.\n",
    "    \"\"\"\n",
    "    critic_results = {}\n",
    "    return critic_results\n",
    "\n",
    "def update_context_schema_v0(critic_results, context_schema, updated_schema):\n",
    "    \"\"\"\n",
    "        critic_results = {\n",
    "            \"schema_name\": {\n",
    "                \"piece_id\": \"label_name\",\n",
    "                \"is_sufficient\": \"Yes/No\",\n",
    "                \"what_is_missing\": \"missing_context\",\n",
    "            },\n",
    "            ...\n",
    "        }\n",
    "        TODO: could consider other schemas beyond \"what is missing\" (e.g., interchangable? is contradicting?)\n",
    "    \"\"\"\n",
    "    ### need to code the `what_is_missing` parts to recognize candidate schemas and update the `context_schema`\n",
    "    last_update_proportion = 0\n",
    "    new_context_schema = context_schema\n",
    "    updated_schema = []\n",
    "\n",
    "    ### TODO: code the `what_is_missing` parts to recognize candidate schemas and update the `context_schema`\n",
    "\n",
    "    return last_update_proportion, new_context_schema, updated_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_videos_approach_1(task, dataset, stopping_criteria=0.1, dummy=\"\"):\n",
    "    taskname = f'{task.replace(\" \", \"_\").lower()}_{dummy}'\n",
    "    path = os.path.join(FRAMEWORK_PATH, taskname)\n",
    "    if os.path.exists(path):\n",
    "        with open(path) as f:\n",
    "            return json.load(f)\n",
    "    \n",
    "    \"\"\"\n",
    "    approach_1_results = {\n",
    "        \"information_units\": {\n",
    "            \"unit_id\": {\n",
    "                \"content\": \"content_text\",\n",
    "                \"content_type\": \"content_type\",\n",
    "                \"instances\": [piece_id_1, piece_id_2, ...],\n",
    "            }\n",
    "        },\n",
    "        \"context_schema\": {\n",
    "            \"schema_name\": {\n",
    "                \"schema\": \"schema_name\",\n",
    "                \"definition\": \"definition\",\n",
    "                \"format\": \"format\",\n",
    "                \"prompt\": \"prompt\",\n",
    "                \"examples\": [\n",
    "                    {\n",
    "                        \"content\": \"content_text\",\n",
    "                        \"label\": \"label_name\",\n",
    "                        \"definition\": \"label_definition\",\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "        },\n",
    "        \"codebook\": {\n",
    "            \"schema_name\": [\n",
    "                {\n",
    "                    \"label\": \"label_name\",\n",
    "                    \"definition\": \"label_definition\",\n",
    "                    \"examples\": [content_1, content_2, ...]\n",
    "                },\n",
    "            ]\n",
    "        },\n",
    "        \"cim\": {\n",
    "            \"schema_name\": {\n",
    "                \"piece_id\": \"label_name\",\n",
    "            }\n",
    "            ...\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    ### build the `information units`\n",
    "    dataset, information_units = build_information_units_v0(dataset, taskname)\n",
    "\n",
    "    ### build the `context_schema`\n",
    "    context_schema = {\n",
    "        ### simple initial schema\n",
    "        \"action\": {\n",
    "            \"schema\": \"action\",\n",
    "            \"definition\": \"The action that is being performend in the the tutorial.\",\n",
    "            \"format\": \"verb\",\n",
    "            \"prompt\": \"Identify the all the actions that are being performed in the tutorial following the format {format}. The actions should be in the same language as the tutorial.\",\n",
    "            \"examples\": [\n",
    "                {\n",
    "                    \"content\": \"Whisk well until the ingredients are well combined.\",\n",
    "                    \"label\": \"whisk\",\n",
    "                    \"definition\": \"The action of mixing the ingredients together.\",\n",
    "                },\n",
    "                {\n",
    "                    \"content\": \"Sift in the flour with baking powder and a pinch of salt.\",\n",
    "                    \"label\": \"sift in\",\n",
    "                    \"definition\": \"The action of sifting the dry ingredients.\",\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    updated_schema = [\"action\"]\n",
    "    codebook = {}\n",
    "    cim = {}\n",
    "\n",
    "    last_update_proportion = 1\n",
    "\n",
    "    # while last_update_proportion > stopping_criteria:\n",
    "    #     ### build the codebook for each schema\n",
    "    #     for schema in updated_schema:\n",
    "    #         codebook[schema] = build_codebook_v0(dataset, context_schema[schema], taskname)\n",
    "    #         cim[schema] = label_according_to_codebook_v0(dataset, codebook[schema], taskname)\n",
    "        \n",
    "    #     last_update_proportion = 0\n",
    "    #     updated_schema = []\n",
    "    #     critic_results = critic_cim_v0(dataset, information_units, codebook, cim)\n",
    "    #     last_update_proportion, new_context_schema, updated_schema = update_context_schema_v0(critic_results, context_schema, updated_schema)\n",
    "\n",
    "    #     context_schema = new_context_schema\n",
    "\n",
    "    approach_1_results = {\n",
    "        \"dataset\": dataset,\n",
    "        \"information_units\": information_units,\n",
    "        \"context_schema\": context_schema,\n",
    "        \"codebook\": codebook,\n",
    "        \"cim\": cim,\n",
    "    }    \n",
    "\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(approach_1_results, f, indent=4)\n",
    "    return approach_1_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def stats(dataset):\n",
    "    non_zero_content_types = [\"Method\", \"Description\", \"Explanation\", \"Supplementary\"]\n",
    "\n",
    "    per_content_type = {}\n",
    "    for video in dataset:\n",
    "        for i, piece in enumerate(video['baseline_results']):\n",
    "            content_type = piece['content_type']\n",
    "            if content_type not in per_content_type:\n",
    "                per_content_type[content_type] = {\n",
    "                    \"count\": 0,\n",
    "                    \"count_p_s\": [],\n",
    "                }\n",
    "            per_content_type[content_type][\"count\"] += 1\n",
    "            per_content_type[content_type][\"count_p_s\"].append(len(piece[\"procedure_segments\"]))\n",
    "            if content_type in non_zero_content_types and len(piece[\"procedure_segments\"]) > 1 and content_type != \"Method\":\n",
    "                ### print prev 2 and the next 2 pieces\n",
    "                to_print = []\n",
    "                for j in range(max(0, i - 2), min(len(video['baseline_results']), i + 3)):\n",
    "                    if i == j:\n",
    "                        to_print.append(f\"[({video['baseline_results'][j]['content_type']}) {video['baseline_results'][j]['content']}]\")\n",
    "                        to_print.append(str(video['baseline_results'][j]['procedure_segments']))\n",
    "                    else:\n",
    "                        to_print.append(f\"({video['baseline_results'][j]['content_type']}) {video['baseline_results'][j]['content']}\")\n",
    "                        to_print.append(str(video['baseline_results'][j]['procedure_segments']))\n",
    "                # print(\"\\n\".join(to_print))\n",
    "                # print(\"-\" * 100)\n",
    "    \n",
    "    to_print = []\n",
    "    for video in dataset:\n",
    "        for piece in video['baseline_results']:\n",
    "            if piece['content_type'] in non_zero_content_types and piece[\"content_type\"] != \"Method\":\n",
    "                print_str = \"\"\n",
    "                print_str += piece['content_type'] + \"\\n\"\n",
    "                print_str += piece['content'] + \"\\n\"\n",
    "                print_str += str(piece['procedure_segments']) + \"\\n\"\n",
    "                print_str += str(piece['procedure_segments_clustered']) + \"\\n\"\n",
    "                print_str += \"\\n\"\n",
    "                to_print.append(print_str)\n",
    "    \n",
    "    ### shuffle to_print\n",
    "    random.shuffle(to_print)\n",
    "    print(\"\\n\".join(to_print))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MUFFIN_TASK = \"Making Muffins\"\n",
    "\n",
    "\"\"\"\n",
    "Make French Toast\t\t\t10 steps / 272 videos\n",
    "Make Irish Coffee\t\t\t5 steps / 248 videos\n",
    "Change a Tire\t\t\t\t11 steps / 119 videos\n",
    "Build (sim.) Floating Shelves\t\t5 steps / 173 videos\n",
    "\"\"\"\n",
    "CROSS_TASK_TASKS = [\n",
    "    \"Change a Tire\",\n",
    "    \"Build (sim.) Floating Shelves\",\n",
    "    \"Make French Toast\",\n",
    "    \"Make Irish Coffee\",\n",
    "]\n",
    "\n",
    "CUSTOM_TASKS = [\n",
    "    ### Food and Entertaining\n",
    "    \"How to Make a Sushi Roll\",\n",
    "    \"How to Make Caramel Apples\",\n",
    "    \"How to Make a Milkshake Without Ice Cream\",\n",
    "    \"How to Grill Steak\",\n",
    "    \"How to Make Scrambled Eggs in a Microwave\",\n",
    "\n",
    "    ### Home and Garden\n",
    "    \"How to Grow Hydrangea from Cuttings\",\n",
    "    \"How to Grow a Pumpkin\",\n",
    "    \"How to Clean Bathroom Tile\",\n",
    "    \"How to Polish Stainless Steel\",\n",
    "    \"How to Clean a Glass Top Stove\",\n",
    "    \"How to Get Rid of a Wasp's Nest\",\n",
    "\n",
    "    # Holidays and Traditions\n",
    "    \"How to Plant a Living Christmas Tree\",\n",
    "\n",
    "    # Sports and Fitness\n",
    "    \"How to Wrap Your Hands for Boxing\",\n",
    "    \"How to Catch Trout\",\n",
    "\n",
    "    # Arts and Entertainment\n",
    "    \"How to Make a Paper Hat\",\n",
    "]\n",
    "\n",
    "\n",
    "def get_dataset(task):\n",
    "    if task == MUFFIN_TASK:\n",
    "        return get_dataset_muffins(task, \"framework_raw\")\n",
    "    elif task in CROSS_TASK_TASKS:\n",
    "        return preprocess_cross_task(task, \"framework_raw\")\n",
    "    elif task in CUSTOM_TASKS:\n",
    "        return preprocess_custom_dataset(task, \"framework_raw\")\n",
    "\n",
    "def print_csv(dataset, ann_key=\"baseline_results\", info_types=[]):\n",
    "    filename = f\"framework_raw_{'_'.join(ann_key.split(' '))}.csv\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(\"video_id,url,original_title,transcript_id,content_type,content,context\\n\")\n",
    "        for v_id, video in enumerate(dataset):\n",
    "            if ann_key not in video:\n",
    "                continue\n",
    "            for i, transcript in enumerate(video[ann_key]):\n",
    "                cur_str = f\"{v_id},{video['url'].strip()},{video['original_title'].strip()},{i},{transcript['content_type'].strip()},\\\"{transcript['content'].strip()}\\\",\"\n",
    "                if \"context_step\" in transcript:\n",
    "                    cur_str += f\"\\\"{transcript['context_step'].strip()}\\\"\"\n",
    "                else:\n",
    "                    if transcript['content_type'] in info_types:\n",
    "                        cur_str += \"\\\"<empty>\\\"\"\n",
    "                    else:\n",
    "                        cur_str += \"\\\"<not-assigned>\\\"\"\n",
    "                cur_str += \"\\n\"\n",
    "                f.write(cur_str)\n",
    "\n",
    "def main(task):\n",
    "    dataset = get_dataset(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fragmentation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.bert import bert_embedding\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def shannon_entropy(distribution):\n",
    "    \"\"\"\n",
    "    Calculate the Shannon entropy of a distribution.\n",
    "    \"\"\"\n",
    "    return -np.sum(distribution * np.log(distribution + 1e-10))\n",
    "\n",
    "def trace_of_covariance(pieces):\n",
    "    if len(pieces) == 0:\n",
    "        return 0\n",
    "    trace = 0\n",
    "    mean = np.mean(pieces, axis=0)\n",
    "    for i in range(len(pieces)):\n",
    "        trace += (pieces[i] - mean) @ (pieces[i] - mean).T\n",
    "    trace /= len(pieces)\n",
    "    return trace\n",
    "\n",
    "def compare_fragmentation(pieces, labels, label_vocab, distance_delta=0.01):\n",
    "    \"\"\"\n",
    "    Check if global fragmentation is higher or lower than the labeled fragmentation.\n",
    "    \"\"\"\n",
    "    similarity_matrix = np.zeros((len(pieces), len(pieces)))\n",
    "    for i in range(len(pieces)):\n",
    "        for j in range(i+1):\n",
    "            similarity_matrix[i, j] = np.dot(pieces[i], pieces[j])\n",
    "            similarity_matrix[j, i] = similarity_matrix[i, j]\n",
    "\n",
    "    distance_buckets = np.arange(0, 1+distance_delta, distance_delta)\n",
    "    \n",
    "    global_distance_distribution = np.zeros(len(distance_buckets))\n",
    "    for i in range(len(pieces)):\n",
    "        for j in range(len(pieces)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            distance = similarity_matrix[i, j]\n",
    "            bucket = 0\n",
    "            for i in range(len(distance_buckets)):\n",
    "                if distance >= distance_buckets[i]:\n",
    "                    bucket = i\n",
    "            global_distance_distribution[bucket] += 1\n",
    "    \n",
    "    ### print # of global distances\n",
    "    print(f\"# of global distances: {np.sum(global_distance_distribution)}\")\n",
    "    print(f\"Trace of covariance: {trace_of_covariance(pieces)}\")\n",
    "\n",
    "    ### normalize global distance distribution\n",
    "    global_distance_distribution = global_distance_distribution / np.sum(global_distance_distribution) * 100\n",
    "    \n",
    "    distance_distribution_per_label = {}\n",
    "\n",
    "    for label in label_vocab:\n",
    "        labeled_distance_distribution = np.zeros(len(distance_buckets))\n",
    "        cur_pieces = []\n",
    "        for i in range(len(pieces)):\n",
    "            if labels[i] != label:\n",
    "                continue\n",
    "            cur_pieces.append(pieces[i])\n",
    "            for j in range(len(pieces)):\n",
    "                if labels[j] != label:\n",
    "                    continue\n",
    "                if i == j:\n",
    "                    continue\n",
    "                distance = similarity_matrix[i, j]\n",
    "                bucket = 0\n",
    "                for i in range(len(distance_buckets)):\n",
    "                    if distance >= distance_buckets[i]:\n",
    "                        bucket = i\n",
    "                labeled_distance_distribution[bucket] += 1\n",
    "        \n",
    "        ### print # of labeled distances\n",
    "        print(f\"# of labeled distances for {label}: {np.sum(labeled_distance_distribution)}\")\n",
    "        print(f\"Trace of covariance for {label}: {trace_of_covariance(cur_pieces)}\")\n",
    "\n",
    "        distance_distribution_per_label[label] = labeled_distance_distribution / np.sum(labeled_distance_distribution) * 100\n",
    "\n",
    "    ### plot the distance distribution in the same plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(distance_buckets * 100, global_distance_distribution, label=\"Global\", linestyle=\"--\")\n",
    "    for label in label_vocab:\n",
    "        plt.plot(distance_buckets * 100, distance_distribution_per_label[label], label=label)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "custom_labels = {\n",
    "    0: \"350F\",\n",
    "    1: \"350F\",\n",
    "    2: \"375F\",\n",
    "    3: \"375F\",\n",
    "    4: \"375F\",\n",
    "    5: \"350F\",\n",
    "    6: \"??\",\n",
    "    7: \"??\",\n",
    "    8: \"425F\",\n",
    "    9: \"??\",\n",
    "    10: \"425F\",\n",
    "}\n",
    "\n",
    "def experiment_analysis(dataset):\n",
    "    \"\"\"\n",
    "    Embed all information pieces in the dataset;\n",
    "    \"\"\"\n",
    "    # content_types_to_include = [\"Method\", \"Description\", \"Explanation\", \"Supplementary\"]\n",
    "    content_types_to_include = [\"Method\"]\n",
    "    pieces = []\n",
    "    labels = []\n",
    "    label_vocab = []\n",
    "    for idx, video in enumerate(dataset):\n",
    "        label = custom_labels[idx]\n",
    "        if label not in label_vocab:\n",
    "            label_vocab.append(label)\n",
    "        for piece in video['baseline_results']:\n",
    "            if piece['content_type'] not in content_types_to_include:\n",
    "                continue\n",
    "            # label = piece['content_type']\n",
    "            # if label not in label_vocab:\n",
    "            #     label_vocab.append(label)\n",
    "            pieces.append(piece['content'])\n",
    "            labels.append(label)\n",
    "    \n",
    "    pieces = bert_embedding(pieces)\n",
    "    compare_fragmentation(pieces, labels, label_vocab, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### output\n",
    "```\n",
    "# of global distances: 110555.0\n",
    "Trace of covariance: 0.6744245930655941\n",
    "# of labeled distances for 350F: 11341.0\n",
    "Trace of covariance for 350F: 0.7017171226929282\n",
    "# of labeled distances for 375F: 8099.0\n",
    "Trace of covariance for 375F: 0.6654950787623723\n",
    "# of labeled distances for ??: 2600.0\n",
    "Trace of covariance for ??: 0.5681160431282193\n",
    "# of labeled distances for 425F: 7224.0\n",
    "Trace of covariance for 425F: 0.6282378954045913\n",
    "```\n",
    "![image](/home/bekzat/starlab/video-analysis/probe/server/static/results/frag-analysis-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "task = MUFFIN_TASK\n",
    "# task = CUSTOM_TASKS[14]\n",
    "# task = CROSS_TASK_TASKS[0]\n",
    "# task = CROSS_TASK_TASKS[1]\n",
    "# task = CUSTOM_TASKS[13]\n",
    "\n",
    "### `Greeting`, `Overview`, `Method`, `Supplementary`, `Explanation`, `Description`, `Conclusion`, and `Miscellaneous`\n",
    "\n",
    "important_information_types = [\"Method\", \"Supplementary\", \"Explanation\", \"Description\"]\n",
    "\n",
    "dataset = get_dataset(task)\n",
    "dataset, information_units = build_information_units_v0(dataset, \"muffins_experiment_1\")\n",
    "\n",
    "context_schema = {\n",
    "    ### simple initial schema\n",
    "    \"action\": {\n",
    "        \"schema\": \"action\",\n",
    "        \"definition\": \"The action that is being performend in the the tutorial.\",\n",
    "        \"format\": \"verb\",\n",
    "        \"prompt\": \"Identify the all the actions that are being performed in the tutorial following the format {format}. The actions should be in the same language as the tutorial.\",\n",
    "        \"examples\": [\n",
    "            {\n",
    "                \"content\": \"Whisk well until the ingredients are well combined.\",\n",
    "                \"label\": \"whisk\",\n",
    "                \"definition\": \"The action of mixing the ingredients together.\",\n",
    "            },\n",
    "            {\n",
    "                \"content\": \"Sift in the flour with baking powder and a pinch of salt.\",\n",
    "                \"label\": \"sift in\",\n",
    "                \"definition\": \"The action of sifting the dry ingredients.\",\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "starlab-video-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
