{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_PATH = \"./static/results/representation/\"\n",
    "\n",
    "### Handle muffins data;\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from preprocess import pre_process_videos\n",
    "\n",
    "from prompts.stupid_experiment_2 import extract_steps\n",
    "\n",
    "def process_muffins_gt_csv():\n",
    "    subgoals_row = 0\n",
    "    ipo_row = 1\n",
    "    url_id = \"URL\"\n",
    "    var_id = \"VARIATION\"\n",
    "    base = \"base\"\n",
    "\n",
    "    gt_path = \"./static/gts/\"\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(gt_path + 'muffins_gt.csv', header=None)\n",
    "\n",
    "    dataset = []\n",
    "\n",
    "    ## concatenat the subgoal row and ipo row\n",
    "    new_headers = df.iloc[[subgoals_row, ipo_row]]\n",
    "    df = df.drop([subgoals_row, ipo_row])\n",
    "\n",
    "    last_url = None\n",
    "    for ri, row in df.iterrows():\n",
    "        general = {}\n",
    "        cur_dataset = []\n",
    "        cur_subgoal = None\n",
    "        for ci in row.index:\n",
    "            col = row[ci]\n",
    "            if pd.isna(col):\n",
    "                continue\n",
    "            cur_header = new_headers.iloc[0, ci]\n",
    "            if pd.isna(cur_header) is False:\n",
    "                cur_subgoal = cur_header\n",
    "            cur_subheader = new_headers.iloc[1, ci]\n",
    "\n",
    "            if pd.isna(cur_subheader):\n",
    "                if pd.isna(cur_header):\n",
    "                    general[var_id] = col\n",
    "                else:\n",
    "                    general[cur_header] = col\n",
    "            else:\n",
    "                cur_dataset.append({\n",
    "                    \"subgoal\": cur_subgoal,\n",
    "                    \"ipo\": cur_subheader,\n",
    "                    \"content\": col,\n",
    "                })\n",
    "        \n",
    "        if url_id not in general or general[url_id] is None:\n",
    "            general[url_id] = last_url\n",
    "        else:\n",
    "            last_url = general[url_id]\n",
    "        for data in cur_dataset:\n",
    "            dataset.append({\n",
    "                **general,\n",
    "                **data,\n",
    "            })\n",
    "\n",
    "    \n",
    "    for data in dataset:\n",
    "        if var_id not in data:\n",
    "            data[var_id] = base\n",
    "        ### reformat the \"content\"\n",
    "        lines = data[\"content\"].split(\"\\n\")\n",
    "        raw_content = data[\"content\"]\n",
    "        information_list = []\n",
    "        detail_list = []\n",
    "        category_list = []\n",
    "        \n",
    "        for line in lines:\n",
    "            if line.strip() == \"\":\n",
    "                continue\n",
    "\n",
    "            if line.strip().startswith(\"- -\"):\n",
    "                ### detail\n",
    "                detail_list.append(line[3:].strip())\n",
    "            elif line.strip().startswith(\"-\"):\n",
    "                ### information\n",
    "                ### for process (the isntructions are roughly divided with `-->`)\n",
    "                information_list.append(line[1:].strip())\n",
    "            else:\n",
    "                ### category\n",
    "                category_list.append(line.strip())\n",
    "\n",
    "        del data[\"content\"]\n",
    "        data[\"category\"] = category_list\n",
    "        data[\"information\"] = information_list\n",
    "        data[\"detail\"] = detail_list\n",
    "        data[\"raw_content\"] = raw_content\n",
    "\n",
    "    # reformat to pandas\n",
    "    df = pd.DataFrame(dataset)\n",
    "    return df\n",
    "\n",
    "def get_muffin_video_transcripts():\n",
    "    library_metadata = {}\n",
    "    with open(\"./metadata.json\") as f:\n",
    "        library_metadata = json.load(f)\n",
    "\n",
    "    task_metadata = library_metadata[\"muffins\"]\n",
    "\n",
    "    muffin_videos = pre_process_videos(task_metadata[\"videos\"])\n",
    "    \n",
    "    transcripts = []\n",
    "    for video in muffin_videos:\n",
    "        url = f\"https://www.youtube.com/watch?v={video.video_id}\"\n",
    "        title = video.metadata[\"title\"]\n",
    "        content = \"\"\n",
    "        for sentence in video.sentences:\n",
    "            content += f\"{sentence['text']}\\n\"\n",
    "\n",
    "        transcripts.append({\n",
    "            \"url\": url,\n",
    "            \"title\": title,\n",
    "            \"content\": content\n",
    "        })\n",
    "    return transcripts\n",
    "\n",
    "def get_muffin_articles():\n",
    "    database_path = \"./static/database/\"\n",
    "    prefix = \"muffin_articles_\"\n",
    "    suffix = \".txt\"\n",
    "    articles = []\n",
    "    \n",
    "    for filename in os.listdir(database_path):\n",
    "        if filename.startswith(prefix) and filename.endswith(suffix):\n",
    "            with open(database_path + filename) as f:\n",
    "                ### read line-by-line\n",
    "                url = f.readline()\n",
    "                title = f.readline()\n",
    "                content = \"\"\n",
    "                for line in f:\n",
    "                    content += line\n",
    "\n",
    "                articles.append({\n",
    "                    \"url\": url,\n",
    "                    \"title\": title,\n",
    "                    \"content\": content\n",
    "                })\n",
    "    return articles\n",
    "\n",
    "def add_steps_to_dataset(dataset, task):\n",
    "\n",
    "    for article in dataset:\n",
    "        tutorial = article[\"title\"] + \"\\n\" + article[\"content\"]\n",
    "        steps = extract_steps(task, tutorial)\n",
    "        article[\"steps\"] = steps\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def get_dataset_muffins(task, dummy=\"\"):\n",
    "    dataset_filepath = f\"{RESULTS_PATH}{task.replace(' ', '_').lower()}_{dummy}.json\"\n",
    "    if os.path.exists(dataset_filepath):\n",
    "        with open(dataset_filepath) as f:\n",
    "            dataset = json.load(f)\n",
    "        return dataset\n",
    "\n",
    "    dataset = get_muffin_articles()\n",
    "    dataset = dataset + get_muffin_video_transcripts()\n",
    "    print(f\"Number of articles: {len(dataset)}\")\n",
    "\n",
    "    dataset = add_steps_to_dataset(dataset, task)\n",
    "\n",
    "    # dataset = add_info_labels_to_dataset(dataset, task)\n",
    "\n",
    "    with open(dataset_filepath, \"w\") as f:\n",
    "        json.dump(dataset, f, indent=4)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Handle CrossTask data;\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "from helpers.video_scripts import extract_transcript\n",
    "\n",
    "def library_cross_task():\n",
    "    library = []\n",
    "    PATH = \"./static/datasets/crosstask/\"\n",
    "    library_path = os.path.join(PATH, \"library.json\")\n",
    "    \n",
    "    if os.path.exists(library_path):\n",
    "        with open(library_path, \"r\") as f:\n",
    "            library = json.load(f)\n",
    "            return library\n",
    "\n",
    "    tasks_path = os.path.join(PATH, \"crosstask_release/tasks_primary.txt\")\n",
    "    videos_path = os.path.join(PATH, \"crosstask_release/videos.csv\")\n",
    "    videos_val_path = os.path.join(PATH, \"crosstask_release/videos_val.csv\")\n",
    "\n",
    "    \"\"\"\n",
    "    Task ID\n",
    "    Task name\n",
    "    URL of corresponding WikiHow page\n",
    "    Number of steps\n",
    "    Ordered list of comma-separated steps of the task\n",
    "    \"\"\"\n",
    "    task_obj_ids = [\"task_id\", \"task_name\", \"url\", \"num_steps\", \"steps\"]\n",
    "\n",
    "    with open(tasks_path) as f:\n",
    "        lines = f.readlines()\n",
    "        for start_idx in range(0, len(lines), 6):\n",
    "            cur_task = {}\n",
    "            finished = False\n",
    "            for idx, task_obj_id in enumerate(task_obj_ids):\n",
    "                if start_idx + idx >= len(lines):\n",
    "                    finished = True\n",
    "                    break\n",
    "                cur_task[task_obj_id] = lines[start_idx + idx].strip()\n",
    "            if finished is False:\n",
    "                library.append(cur_task)\n",
    "\n",
    "    for task in library:\n",
    "        task[\"steps\"] = task[\"steps\"].split(\",\")\n",
    "        task[\"videos\"] = []\n",
    "\n",
    "    for videos_path in [videos_path, videos_val_path]:\n",
    "        with open(videos_path) as f:\n",
    "            reader = csv.reader(f)\n",
    "            for row in reader:\n",
    "                task_id = row[0]\n",
    "                video_id = row[1]\n",
    "                video_url = row[2]\n",
    "                for task in library:\n",
    "                    if task[\"task_id\"] == task_id:\n",
    "                        task[\"videos\"].append({\n",
    "                            \"video_id\": video_id,\n",
    "                            \"video_url\": video_url,\n",
    "                        })\n",
    "\n",
    "    def get_language(video_subtitles_path):\n",
    "        with open(video_subtitles_path) as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                if \"Language:\" in line:\n",
    "                    return line.split(\":\")[1].strip()\n",
    "        return None\n",
    "\n",
    "\n",
    "    SUBTITLES_PATH = os.path.join(PATH, \"subtitles\")\n",
    "    for task in library:\n",
    "        for video in task[\"videos\"]:\n",
    "            video_id = video[\"video_id\"]\n",
    "            video_subtitles_path = os.path.join(SUBTITLES_PATH, f\"{video_id}.vtt\")\n",
    "            video[\"subtitles\"] = []\n",
    "\n",
    "            language = get_language(video_subtitles_path)\n",
    "            if language == \"en\":\n",
    "                video[\"subtitles\"] = extract_transcript(video_subtitles_path, None)\n",
    "\n",
    "    ANNOTATIONS_PATH = os.path.join(PATH, \"crosstask_release/annotations/\")\n",
    "\n",
    "    for task in library:\n",
    "        for video in task[\"videos\"]:\n",
    "            video[\"annotations\"] = []\n",
    "            annotation_path = os.path.join(ANNOTATIONS_PATH, f\"{task['task_id']}_{video['video_id']}.csv\")\n",
    "            if os.path.exists(annotation_path):\n",
    "                with open(annotation_path) as f:\n",
    "                    reader = csv.reader(f)\n",
    "                    for row in reader:\n",
    "                        video[\"annotations\"].append({\n",
    "                            \"step\": float(row[0]),\n",
    "                            \"start\": float(row[1]),\n",
    "                            \"end\": float(row[2]),\n",
    "                        })\n",
    "            else:\n",
    "                print(f\"No annotation found for {task['task_id']}_{video['video_id']}\")\n",
    "\n",
    "    ### label subtitles with step\n",
    "    for task in library:\n",
    "        for video in task[\"videos\"]:\n",
    "            annotated_subtitles = []\n",
    "            for subtitle in video[\"subtitles\"]:\n",
    "                cur_step = None\n",
    "                for annotation in video[\"annotations\"]:\n",
    "                    if subtitle[\"start\"] >= annotation[\"start\"] and subtitle[\"finish\"] <= annotation[\"end\"]:\n",
    "                        cur_step = task[\"steps\"][int(annotation[\"step\"]) - 1]\n",
    "                        break\n",
    "                annotated_subtitles.append({\n",
    "                    **subtitle,\n",
    "                    \"step\": cur_step,\n",
    "                })\n",
    "            video[\"subtitles\"] = annotated_subtitles\n",
    "\n",
    "    ### restructure to be similar to the `dataset`\n",
    "\n",
    "    ### save library as json\n",
    "    with open(library_path, \"w\") as f:\n",
    "        json.dump(library, f, indent=4)\n",
    "\n",
    "\n",
    "def get_dataset_cross_task(task):\n",
    "    \"\"\"\n",
    "    return dataset with the given task with a structure similar to the `dataset`\n",
    "    \"\"\"\n",
    "    library = library_cross_task()\n",
    "    dataset = []\n",
    "    for _task in library:\n",
    "        if _task[\"task_name\"] == task:\n",
    "            for video in _task[\"videos\"]:\n",
    "                content = \"\"\n",
    "                for subtitle in video[\"subtitles\"]:\n",
    "                    content += f\"{subtitle['text']}\\n\"\n",
    "                dataset.append({\n",
    "                    \"id\": video[\"video_id\"],\n",
    "                    \"url\": video[\"video_url\"],\n",
    "                    \"title\": task,\n",
    "                    \"content\": content,\n",
    "                    \"steps\": [],\n",
    "                    \"ipo\": [],\n",
    "                    \"processed_ipos\": [],\n",
    "                })\n",
    "\n",
    "    ### check if content is enough\n",
    "    filtered_dataset = []\n",
    "    for article in dataset:\n",
    "        if len(article[\"content\"]) < 100:\n",
    "            continue\n",
    "        filtered_dataset.append(article)\n",
    "    dataset = filtered_dataset\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def preprocess_cross_task(task, dummy=\"\"):\n",
    "    dataset_filepath = f\"{RESULTS_PATH}{task.replace(' ', '_').lower()}_{dummy}.json\"\n",
    "    if os.path.exists(dataset_filepath):\n",
    "        with open(dataset_filepath) as f:\n",
    "            dataset = json.load(f)\n",
    "        return dataset\n",
    "\n",
    "\n",
    "    dataset = get_dataset_cross_task(task)\n",
    "    print(f\"Dataset for {task}: {len(dataset)}\")\n",
    "    dataset = add_steps_to_dataset(dataset, task)\n",
    "\n",
    "    with open(dataset_filepath, \"w\") as f:\n",
    "        json.dump(dataset, f, indent=4)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPO_KEYS = [\"inputs\", \"outputs\", \"methods\"]\n",
    "# INFORMATION_KEYS = [\"description\", \"explanation\", \"tips\", \"alternatives\"]\n",
    "INFORMATION_KEYS = [\"description\", \"instruction\", \"explanations\", \"tips\"]\n",
    "\n",
    "PRIMARY_INFORMATION_KEYS = [\"description\", \"instruction\"]\n",
    "SUPP_INFORMATION_KEYS = [\"explanations\", \"tips\"]\n",
    "\n",
    "KEY_LEVELS = {\n",
    "    \"r\": 0,\n",
    "    \"phase\": 1,\n",
    "    \"step\": 2,\n",
    "    \"ipo\": 3,\n",
    "    \"element\": 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pipeline\n",
    "\n",
    "from helpers.bert import hierarchical_clustering\n",
    "\n",
    "from prompts.stupid_experiment_2 import aggregate_steps_stupid\n",
    "from prompts.stupid_experiment_2 import extract_ipos\n",
    "from prompts.stupid_experiment_2 import taxonomize_ipos_stupid\n",
    "from prompts.stupid_experiment_2 import extract_information_per_ipo_stupid\n",
    "from prompts.stupid_experiment_2 import cluster_information_stupid\n",
    "\n",
    "def aggregate_hierarchical(items, task, distance_threshold=0.2):\n",
    "\n",
    "    clusters = hierarchical_clustering(items, embedding_method=\"bert\", linkage=\"average\", n_clusters=None, distance_threshold=distance_threshold)\n",
    "\n",
    "    # mappings = {}\n",
    "    # for i, cluster in enumerate(clusters):\n",
    "    #     if cluster not in mappings:\n",
    "    #         mappings[cluster] = []\n",
    "    #     mappings[cluster].append(items[i])\n",
    "\n",
    "    # print(json.dumps(mappings, indent=4))\n",
    "    return clusters\n",
    "\n",
    "def construct_step_taxonomy(dataset, task, dummy=\"\"):\n",
    "    step_taxonomy_filepath = f\"{RESULTS_PATH}{task.replace(' ', '_').lower()}_taxonomy_{dummy}.json\"\n",
    "    if os.path.exists(step_taxonomy_filepath):\n",
    "        with open(step_taxonomy_filepath) as f:\n",
    "            taxonomy = json.load(f)\n",
    "        return taxonomy\n",
    "    \n",
    "    all_steps = []\n",
    "    for article in dataset:\n",
    "        for step in article[\"steps\"]:\n",
    "            all_steps.append({\n",
    "                \"step\": step[\"step\"],\n",
    "                \"description\": step[\"description\"],\n",
    "                \"original_tutorial\": article[\"url\"],\n",
    "            })\n",
    "    ### hierarchical clustering\n",
    "    # taxonomy = aggregate_hierarchical(all_steps, task)\n",
    "\n",
    "    ### LLM-based stupid aggregation\n",
    "    taxonomy = aggregate_steps_stupid(task, all_steps)\n",
    "\n",
    "    with open(step_taxonomy_filepath, \"w\") as f:\n",
    "        json.dump(taxonomy, f, indent=4)\n",
    "    return taxonomy\n",
    "\n",
    "def extract_ipos_stupid(dataset, taxonomy, task, dummy=\"\"):\n",
    "    \"\"\"\n",
    "    TODO: can potentially simplify the input-output-instruction extraction, since we are taxonomizing anyway...\n",
    "    \"\"\"\n",
    "    ipos_filepath = f\"{RESULTS_PATH}{task.replace(' ', '_').lower()}_ipos_{dummy}.json\"\n",
    "    if os.path.exists(ipos_filepath):\n",
    "        with open(ipos_filepath) as f:\n",
    "            dataset = json.load(f)\n",
    "        return dataset\n",
    "    \n",
    "    for article in dataset:\n",
    "        tutorial = article[\"title\"] + \"\\n\" + article[\"content\"]\n",
    "        article[\"ipo\"] = extract_ipos(task, taxonomy, tutorial)\n",
    "    \n",
    "    with open(ipos_filepath, \"w\") as f:\n",
    "        json.dump(dataset, f, indent=4)\n",
    "    return dataset\n",
    "\n",
    "# def reduce_variance_ipos(dataset, taxonomy, task):\n",
    "#     \"\"\"\n",
    "#     just reduce the variance with hierarchical clustering\n",
    "#     \"\"\"\n",
    "\n",
    "#     ipo_per_step = {}\n",
    "#     for step in taxonomy:\n",
    "#         ipo_per_step[step[\"step\"]] = {\n",
    "#             \"phase\": step[\"phase\"],\n",
    "#             \"tutorials\": [],\n",
    "#         }\n",
    "\n",
    "#     for article in dataset:\n",
    "#         for step_info in article[\"ipo\"]:\n",
    "#             step = step_info[\"step\"]\n",
    "#             if step not in ipo_per_step:\n",
    "#                 print(f\"Error: Step {step} not found in taxonomy\")\n",
    "#                 continue\n",
    "#             cur_entry = {}\n",
    "#             for ipo_key in IPO_KEYS:\n",
    "#                 if ipo_key not in step_info:\n",
    "#                     print(f\"Error: {ipo_key} not found in step {step}\")\n",
    "#                     continue\n",
    "#                 cur_entry[ipo_key] = step_info[ipo_key]\n",
    "#             ipo_per_step[step][\"tutorials\"].append(cur_entry)\n",
    "\n",
    "#     ipo_taxonomy = {}\n",
    "\n",
    "#     for step in ipo_per_step:\n",
    "#         subtask = ipo_per_step[step][\"phase\"] + \": \" + step\n",
    "#         tutorials = ipo_per_step[step][\"tutorials\"]\n",
    "#         # ipo_taxonomy[step] = taxonomize_ipos_stupid(task, tutorials, subtask)\n",
    "\n",
    "#         objects = []\n",
    "#         instruction_sets = []\n",
    "#         for tutorial in tutorials:\n",
    "#             if len(tutorial[\"inputs\"]) > 0:\n",
    "#                 objects.extend(tutorial[\"inputs\"])\n",
    "#             if len(tutorial[\"outputs\"]) > 0:\n",
    "#                 objects.extend(tutorial[\"outputs\"])\n",
    "#             if len(tutorial[\"methods\"]) > 0:\n",
    "#                 instruction_sets.append(\"; \".join(tutorial[\"methods\"]))\n",
    "\n",
    "#         aggregate_hierarchical(objects, subtask, distance_threshold=0.2)\n",
    "#         aggregate_hierarchical(instruction_sets, subtask, distance_threshold=0.2)\n",
    "\n",
    "def taxonomize_ipos(dataset, taxonomy, task, dummy=\"\"):\n",
    "    ipo_taxonomy_filepath = f\"{RESULTS_PATH}{task.replace(' ', '_').lower()}_ipo_taxonomy_{dummy}.json\"\n",
    "    if os.path.exists(ipo_taxonomy_filepath):\n",
    "        with open(ipo_taxonomy_filepath) as f:\n",
    "            ipo_taxonomy = json.load(f)\n",
    "        return ipo_taxonomy\n",
    "\n",
    "    ipo_per_step = {}\n",
    "    for step in taxonomy:\n",
    "        ipo_per_step[step[\"step\"]] = {\n",
    "            \"phase\": step[\"phase\"],\n",
    "            \"tutorials\": [],\n",
    "        }\n",
    "\n",
    "    for article in dataset:\n",
    "        for step_info in article[\"ipo\"]:\n",
    "            step = step_info[\"step\"]\n",
    "            if step not in ipo_per_step:\n",
    "                print(f\"Error: Step {step} not found in taxonomy\")\n",
    "                continue\n",
    "            cur_entry = {}\n",
    "            for ipo_key in IPO_KEYS:\n",
    "                if ipo_key not in step_info:\n",
    "                    print(f\"Error: {ipo_key} not found in step {step}\")\n",
    "                    continue\n",
    "                cur_entry[ipo_key] = step_info[ipo_key]\n",
    "            ipo_per_step[step][\"tutorials\"].append(cur_entry)\n",
    "\n",
    "    ipo_taxonomy = {}\n",
    "\n",
    "    for step in ipo_per_step:\n",
    "        subtask = ipo_per_step[step][\"phase\"] + \": \" + step\n",
    "        tutorials = ipo_per_step[step][\"tutorials\"]\n",
    "        ipo_taxonomy[step] = taxonomize_ipos_stupid(task, tutorials, subtask)\n",
    "\n",
    "    with open(ipo_taxonomy_filepath, \"w\") as f:\n",
    "        json.dump(ipo_taxonomy, f, indent=4)\n",
    "    return ipo_taxonomy\n",
    "\n",
    "\n",
    "    ### TODO: Try each component separately\n",
    "    # for article in dataset:\n",
    "    #     for step_info in article[\"ipo\"]:\n",
    "    #         step = step_info[\"step\"]\n",
    "    #         if step not in ipo_per_step:\n",
    "    #             print(f\"Error: Step {step} not found in taxonomy\")\n",
    "    #             continue\n",
    "    #         for ipo_key in IPO_KEYS:\n",
    "    #             if ipo_key not in step_info:\n",
    "    #                 print(f\"Error: {ipo_key} not found in step {step}\")\n",
    "    #                 continue\n",
    "    #             if ipo_key not in ipo_per_step[step]:\n",
    "    #                 ipo_per_step[step][ipo_key] = []\n",
    "    #             ipo_per_step[step][ipo_key].append({\n",
    "    #                 \"present\": step_info[\"present\"],\n",
    "    #                 \"set\": step_info[ipo_key],\n",
    "    #                 \"original_tutorial\": article[\"url\"],\n",
    "    #             })\n",
    "    \n",
    "    # ipo_taxonomy_per_step = {}\n",
    "\n",
    "    # for step in ipo_per_step:\n",
    "    #     if step not in ipo_taxonomy_per_step:\n",
    "    #         ipo_taxonomy_per_step[step] = {\n",
    "    #             \"phase\": ipo_per_step[step][\"phase\"],\n",
    "    #         }\n",
    "\n",
    "    #     # for ipo_key in IPO_KEYS:\n",
    "    #     #     if ipo_key not in ipo_taxonomy_per_step[step]:\n",
    "    #     #         ipo_taxonomy_per_step[step][ipo_key] = []\n",
    "    #     #     if ipo_key not in ipo_per_step[step]:\n",
    "    #     #         ipo_per_step[step][ipo_key] = []\n",
    "    #     #         continue\n",
    "    #     #     ### taxonomize the sets: enter the entire lists with some context and try to aggregate wrt other parts of the IPO\n",
    "\n",
    "def extract_information_per_ipo(dataset, step_taxonomy, ipo_taxonomy, task, dummy=\"\"):\n",
    "    dataset_filepath = f\"{RESULTS_PATH}{task.replace(' ', '_').lower()}_ipo_information_{dummy}.json\"\n",
    "    if os.path.exists(dataset_filepath):\n",
    "        with open(dataset_filepath) as f:\n",
    "            dataset = json.load(f)\n",
    "        return dataset\n",
    "    \n",
    "    step_to_subtask = {}\n",
    "    for step in step_taxonomy:\n",
    "        step_to_subtask[step[\"step\"]] = f\"{step['phase']}: {step['step']}\"\n",
    "\n",
    "    async_calls = []\n",
    "\n",
    "    for article in dataset:\n",
    "        article[\"ipo_information\"] = []\n",
    "        for step_info in article[\"ipo\"]:\n",
    "            step = step_info[\"step\"]\n",
    "            if step not in ipo_taxonomy:\n",
    "                print(f\"Error: Step {step} not found in ipo_taxonomy\")\n",
    "                continue\n",
    "            if step not in step_to_subtask:\n",
    "                print(f\"Error: Step {step} not found in step_to_subtask\")\n",
    "                continue\n",
    "            tutorial = article[\"title\"] + \"\\n\" + article[\"content\"]\n",
    "            subtask = step_to_subtask[step]\n",
    "            step_ipo_taxonomy = ipo_taxonomy[step]\n",
    "            cur_information = extract_information_per_ipo_stupid(task, step_ipo_taxonomy, tutorial, subtask)\n",
    "            article[\"ipo_information\"].append({\n",
    "                \"step\": step,\n",
    "                ### ideally add the phase?\n",
    "                \"subtask\": subtask,\n",
    "                **cur_information,\n",
    "            })\n",
    "            # async_calls.append((task, step_ipo_taxonomy, tutorial, subtask))\n",
    "\n",
    "    # print(f\"Number of async calls: {len(async_calls)}\")\n",
    "    # async def process_call(task, step_ipo_taxonomy, tutorial, subtask):\n",
    "    #     return await extract_information_per_ipo_stupid(task, step_ipo_taxonomy, tutorial, subtask)\n",
    "\n",
    "    # loop = asyncio.get_event_loop()\n",
    "    # results = await asyncio.gather(*[process_call(task, step_ipo_taxonomy, tutorial, subtask) for task, step_ipo_taxonomy, tutorial, subtask in async_calls])\n",
    "    # for i, article in enumerate(dataset):\n",
    "    #     for step_info in article[\"ipo\"]:\n",
    "    #         step = step_info[\"step\"]\n",
    "    #         if step not in ipo_taxonomy:\n",
    "    #             print(f\"Error: Step {step} not found in ipo_taxonomy\")\n",
    "    #             continue\n",
    "    #         if step not in step_to_subtask:\n",
    "    #             print(f\"Error: Step {step} not found in step_to_subtask\")\n",
    "    #             continue\n",
    "    #         subtask = step_to_subtask[step]\n",
    "    #         cur_information = results[i]\n",
    "    #         article[\"ipo_information\"].append({\n",
    "    #             \"step\": step,\n",
    "    #             ### ideally add the phase?\n",
    "    #             \"subtask\": subtask,\n",
    "    #             **cur_information,\n",
    "    #         })\n",
    "\n",
    "    with open(dataset_filepath, \"w\") as f:\n",
    "        json.dump(dataset, f, indent=4)\n",
    "    return dataset\n",
    "\n",
    "def extract_task_representation(dataset, step_taxonomy, ipo_taxonomy, task, agg_approach, agg_distance_threshold, dummy=\"\"):\n",
    "    representation_filepath = f\"{RESULTS_PATH}{task.replace(' ', '_').lower()}_representation_{dummy}.json\"\n",
    "\n",
    "    if os.path.exists(representation_filepath):\n",
    "        with open(representation_filepath) as f:\n",
    "            representation = json.load(f)\n",
    "        return representation\n",
    "    \n",
    "    step_to_phase = {}\n",
    "    for step in step_taxonomy:\n",
    "        step_to_phase[step[\"step\"]] = step[\"phase\"]\n",
    "\n",
    "    ### covered articles; todo: remove later\n",
    "    covered_articles = {}\n",
    "\n",
    "    representation = {} # phase&step --> ipo_taxonomy --> elements --> information\n",
    "\n",
    "    ### extract the task representation\n",
    "    for article in dataset:\n",
    "        if article[\"url\"] in covered_articles:\n",
    "            continue\n",
    "        covered_articles[article[\"url\"]] = 1\n",
    "        for step_info in article[\"ipo_information\"]:\n",
    "            step = step_info[\"step\"]\n",
    "            phase = step_to_phase[step]\n",
    "            if phase not in representation:\n",
    "                representation[phase] = {}\n",
    "            if step not in representation[phase]:\n",
    "                representation[phase][step] = {}\n",
    "            for ipo_key in IPO_KEYS:\n",
    "                if ipo_key not in step_info:\n",
    "                    continue\n",
    "                if ipo_key not in representation[phase][step]:\n",
    "                    representation[phase][step][ipo_key] = {}\n",
    "                for element in step_info[ipo_key]:\n",
    "                    element_name = element[\"name\"]\n",
    "                    is_present = element[\"present\"]\n",
    "                    if is_present is False:\n",
    "                        continue\n",
    "                    if element_name not in representation[phase][step][ipo_key]:\n",
    "                        representation[phase][step][ipo_key][element_name] = []\n",
    "                    for information_key in INFORMATION_KEYS:\n",
    "                        if information_key not in element:\n",
    "                            continue\n",
    "                        if isinstance(element[information_key], list):\n",
    "                            for text in element[information_key]:\n",
    "                                representation[phase][step][ipo_key][element_name].append({\n",
    "                                    \"url\": article[\"url\"],\n",
    "                                    \"content\": text,\n",
    "                                    \"type\": information_key\n",
    "                                })\n",
    "                        else:\n",
    "                            representation[phase][step][ipo_key][element_name].append({\n",
    "                                \"url\": article[\"url\"],\n",
    "                                \"content\": element[information_key],\n",
    "                                \"type\": information_key,\n",
    "                            })\n",
    "                    \n",
    "    def aggregate_information(items, key, context):\n",
    "        contents = []\n",
    "        for item in items:\n",
    "            contents.append(item[\"content\"])\n",
    "        if agg_approach == \"clustering\":\n",
    "            clusters = aggregate_hierarchical(contents, context, distance_threshold=INFORMATION_AGGREGATION_DISTANCE_THRESHOLD)\n",
    "        elif agg_approach == \"llm-based\":\n",
    "            clusters = cluster_information_stupid(task, contents, context, key)\n",
    "\n",
    "        result = []\n",
    "        information_taxonomy = {}\n",
    "        for idx, cluster in enumerate(clusters):\n",
    "            if cluster not in information_taxonomy:\n",
    "                information_taxonomy[cluster] = []\n",
    "            information_taxonomy[cluster].append(items[idx])\n",
    "\n",
    "        for cluster in information_taxonomy:\n",
    "            cur_items = information_taxonomy[cluster]\n",
    "            rep = {\n",
    "                \"type\": key,\n",
    "                \"content\": cur_items[0][\"content\"],\n",
    "                \"items\": cur_items\n",
    "            }\n",
    "            result.append(rep)\n",
    "        return result\n",
    "\n",
    "\n",
    "    ### cluster similar element information\n",
    "    for phase in representation:\n",
    "        for step in representation[phase]:\n",
    "            for ipo_key in representation[phase][step]:\n",
    "                for element in representation[phase][step][ipo_key]:\n",
    "                    items = representation[phase][step][ipo_key][element]\n",
    "                    ### cluster the descriptions\n",
    "                    ### TODO: can do this across each level for each type of info and see the overlap???\n",
    "                    context = f\"{phase} -> {step} -> {ipo_key} -> {element}\"\n",
    "                    library = []\n",
    "                    for information_key in INFORMATION_KEYS:\n",
    "                        ### some are str and some are list[str]\n",
    "                        actual_items = []\n",
    "                        for item in items:\n",
    "                            if item[\"type\"] == information_key:\n",
    "                                actual_items.append({\n",
    "                                    \"url\": item[\"url\"],\n",
    "                                    \"content\": item[\"content\"]\n",
    "                                })\n",
    "                        library.extend(\n",
    "                            aggregate_information(actual_items, information_key, context)\n",
    "                        )\n",
    "                    ### sort library elements in the decreasing order of the number of items\n",
    "                    library.sort(key=lambda x: len(x[\"items\"]), reverse=True)\n",
    "                    representation[phase][step][ipo_key][element] = library\n",
    "                        \n",
    "    ### save the representation\n",
    "    with open(representation_filepath, \"w\") as f:\n",
    "        json.dump(representation, f, indent=4)\n",
    "    return representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Analysis\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analysis_count_summary(\n",
    "    representation, tutorial_urls, url_to_nice_name,\n",
    "    key_levels_to_draw=[],\n",
    "    information_keys_to_draw=[],\n",
    "):\n",
    "    def get_statistics_per_information_key(counter_list):\n",
    "        \"\"\"\n",
    "        calculate the average and std\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        for information_key in INFORMATION_KEYS:\n",
    "            results[information_key] = {}\n",
    "            statistics = {}\n",
    "            for counter in counter_list:\n",
    "                if information_key not in counter:\n",
    "                    continue\n",
    "                for key in counter[information_key]:\n",
    "                    if key not in statistics:\n",
    "                        statistics[key] = []\n",
    "                    statistics[key].append(counter[information_key][key])\n",
    "\n",
    "            for key in statistics:\n",
    "                results[information_key][key] = {\n",
    "                    \"sum\": sum(statistics[key]),\n",
    "                    \"average\": np.mean(statistics[key]),\n",
    "                    \"std\": np.std(statistics[key]),\n",
    "                }\n",
    "        return results\n",
    "    \n",
    "    all_statistics = []\n",
    "    procedure_counter = []\n",
    "    for phase in representation:\n",
    "        phase_counter = []\n",
    "        for step in representation[phase]:\n",
    "            step_counter = []\n",
    "            for ipo_key in representation[phase][step]:\n",
    "                ipo_key_counter = []\n",
    "                for element in representation[phase][step][ipo_key]:\n",
    "                    element_counter = {}\n",
    "                    for information_key in INFORMATION_KEYS:\n",
    "                        per_information_key = []\n",
    "                        for info in representation[phase][step][ipo_key][element]:\n",
    "                                if info[\"type\"] == information_key:\n",
    "                                    per_information_key.append(info)\n",
    "\n",
    "                        clustered = len(per_information_key)\n",
    "                        \n",
    "                        ### total number of information pieces\n",
    "                        total = 0\n",
    "                        for rep in per_information_key:\n",
    "                            total += len(rep[\"items\"])\n",
    "\n",
    "                        ### presence of information in each tutorial\n",
    "                        tutorial_coverages = {}\n",
    "                        for url in tutorial_urls:\n",
    "                            tutorial_coverages[url] = 0\n",
    "                        for rep in per_information_key:\n",
    "                            for item in rep[\"items\"]:\n",
    "                                if item[\"url\"] not in tutorial_urls:\n",
    "                                    continue\n",
    "                                tutorial_coverages[item[\"url\"]] = 1\n",
    "                            \n",
    "                        element_counter[information_key] = {\n",
    "                            \"clustered\": clustered,\n",
    "                            \"total\": total,\n",
    "                            **tutorial_coverages,\n",
    "                        }\n",
    "                    cur_key = f\"r.{phase}.{step}.{ipo_key}.{element}\"\n",
    "                    all_statistics.append({\n",
    "                        \"key\": cur_key,\n",
    "                        \"statistics\": get_statistics_per_information_key([element_counter]),\n",
    "                    })\n",
    "                    ipo_key_counter.append(element_counter)\n",
    "                cur_key = f\"r.{phase}.{step}.{ipo_key}\"\n",
    "                all_statistics.append({\n",
    "                    \"key\": cur_key,\n",
    "                    \"statistics\": get_statistics_per_information_key(ipo_key_counter),\n",
    "                })\n",
    "                step_counter.extend(ipo_key_counter)\n",
    "            cur_key = f\"r.{phase}.{step}\"\n",
    "            all_statistics.append({\n",
    "                \"key\": cur_key,\n",
    "                \"statistics\": get_statistics_per_information_key(step_counter),\n",
    "            })\n",
    "            phase_counter.extend(step_counter)\n",
    "        cur_key = f\"r.{phase}\"\n",
    "        all_statistics.append({\n",
    "            \"key\": cur_key,\n",
    "            \"statistics\": get_statistics_per_information_key(phase_counter),\n",
    "        })\n",
    "        procedure_counter.extend(phase_counter)\n",
    "    all_statistics.append({\n",
    "        \"key\": \"r\",\n",
    "        \"statistics\": get_statistics_per_information_key(procedure_counter),\n",
    "    })\n",
    "\n",
    "    meaningful_stats = []\n",
    "    for information_key in INFORMATION_KEYS:\n",
    "        for all_stat in all_statistics:\n",
    "                statistics = all_stat[\"statistics\"]\n",
    "                key = all_stat[\"key\"]\n",
    "                if information_key not in statistics:\n",
    "                    continue\n",
    "                \n",
    "                cur_stats = {}\n",
    "                for url in tutorial_urls:\n",
    "                    total_info = statistics[information_key][\"total\"][\"sum\"]\n",
    "                    clustered_info = statistics[information_key][\"clustered\"][\"sum\"]\n",
    "                    if url not in statistics[information_key] or statistics[information_key][url][\"sum\"] == 0:\n",
    "                        cur_stats[url] = {\n",
    "                            \"coverage\": 0,\n",
    "                            \"of_total\": 0,\n",
    "                            \"total\": total_info,\n",
    "                            \"of_clustered\": 0,\n",
    "                            \"clustered\": clustered_info,\n",
    "                        }\n",
    "                        continue\n",
    "\n",
    "                    point_coverage = statistics[information_key][url][\"sum\"]\n",
    "                    cur_stats[url] = {\n",
    "                        \"coverage\": point_coverage,\n",
    "                        \"of_total\": round(point_coverage / total_info * 100, 2),\n",
    "                        \"total\": total_info,\n",
    "                        \"of_clustered\": round(point_coverage / clustered_info * 100, 2),\n",
    "                        \"clustered\": clustered_info,\n",
    "                    } \n",
    "                meaningful_stats.append({\n",
    "                    \"key\": key,\n",
    "                    \"information_key\": information_key,\n",
    "                    \"coverages\": cur_stats,\n",
    "                })\n",
    "\n",
    "    def filter_by_key_level(key_level, information_key):\n",
    "        num_of_dots = KEY_LEVELS[key_level]\n",
    "        filtered = []\n",
    "        for stat in meaningful_stats:\n",
    "            if len(stat[\"key\"].split(\".\")) != num_of_dots + 1:\n",
    "                continue\n",
    "            if information_key != stat[\"information_key\"]:\n",
    "                continue\n",
    "            filtered.append(stat)\n",
    "        return filtered\n",
    "\n",
    "    ### draw bar chart for each key level & information key where each bar is a url and the height is the coverage `of_clustered`\n",
    "\n",
    "    data_to_draw = {}\n",
    "    for key_level in KEY_LEVELS:\n",
    "        for information_key in INFORMATION_KEYS:\n",
    "            filtered = filter_by_key_level(key_level, information_key)\n",
    "            per_tutorial = {}\n",
    "            max_clustered = []\n",
    "            for stat in filtered:\n",
    "                clustered = None\n",
    "                for url in tutorial_urls:\n",
    "                    if url not in stat[\"coverages\"]:\n",
    "                        continue\n",
    "                    cropped_url = url_to_nice_name[url]\n",
    "                    if cropped_url not in per_tutorial:\n",
    "                        per_tutorial[cropped_url] = []\n",
    "                    per_tutorial[cropped_url].append(stat[\"coverages\"][url][\"of_clustered\"])\n",
    "                    if clustered is not None and clustered != stat[\"coverages\"][url][\"clustered\"]:\n",
    "                        print(f\"clustered is not the same for {url}\")\n",
    "                    clustered = stat[\"coverages\"][url][\"clustered\"]\n",
    "                max_clustered.append(clustered)\n",
    "            for url in per_tutorial:\n",
    "                per_tutorial[url] = np.mean(per_tutorial[url])\n",
    "                \n",
    "            \n",
    "            data_to_draw[f\"{key_level}.{information_key}\"] = {\n",
    "                \"data\": per_tutorial,\n",
    "                \"type\": \"percentage\",\n",
    "                \"total\": 100,\n",
    "                # \"type\": \"count\",\n",
    "                # \"total\": max_clustered,\n",
    "                \"add\": f\"L={len(max_clustered)}; {round(np.mean(max_clustered), 0)} Â± {round(np.std(max_clustered), 0)}\",\n",
    "            }\n",
    "\n",
    "    fig, axs = plt.subplots(len(key_levels_to_draw), len(information_keys_to_draw), figsize=(15, 15))\n",
    "    ### add padding between plots\n",
    "    fig.subplots_adjust(wspace=0.5, hspace=1)\n",
    "\n",
    "    for lvl_idx, key_level in enumerate(key_levels_to_draw):\n",
    "        for info_idx, information_key in enumerate(information_keys_to_draw):\n",
    "            key = f\"{key_level}.{information_key}\"\n",
    "            data = data_to_draw[key][\"data\"]\n",
    "            bar_type = data_to_draw[key][\"type\"]\n",
    "            total = data_to_draw[key][\"total\"]\n",
    "            add = data_to_draw[key][\"add\"]\n",
    "\n",
    "            # bar_keys = list(data.keys())\n",
    "            # bar_keys = sorted(bar_keys, key=lambda x: int(x[1:]))\n",
    "            # bar_values = [data[bar_key] for bar_key in bar_keys]\n",
    "            # axs[lvl_idx, info_idx].bar(bar_keys, bar_values)\n",
    "\n",
    "\n",
    "            ### probably better to draw the distribution of # of tutorials for a particular coverage bracket\n",
    "            bracket_size = 1 #percent\n",
    "            bracketed_data = {}\n",
    "            for bracket_idx in range(0, 20):\n",
    "                bracketed_data[bracket_idx * bracket_size] = 0\n",
    "            for url in data:\n",
    "                coverage = data[url]\n",
    "                bracket_id = int(coverage / bracket_size) * bracket_size\n",
    "                if bracket_id not in bracketed_data:\n",
    "                    bracketed_data[bracket_id] = 0\n",
    "                bracketed_data[bracket_id] += 1\n",
    "\n",
    "            bracketed_keys = list(bracketed_data.keys())\n",
    "            bracketed_keys = sorted(bracketed_keys)\n",
    "            bracketed_values = [bracketed_data[bracketed_key] for bracketed_key in bracketed_keys]\n",
    "\n",
    "            axs[lvl_idx, info_idx].bar(bracketed_keys, bracketed_values)\n",
    "            axs[lvl_idx, info_idx].set_title(key)\n",
    "            axs[lvl_idx, info_idx].set_ylim(0, len(tutorial_urls))\n",
    "            # axs[lvl_idx, info_idx].set_ylim(0, 5)\n",
    "\n",
    "            axs[lvl_idx, info_idx].text(0.95, 0.95, f\"{add}\", ha=\"right\", va=\"top\", transform=axs[lvl_idx, info_idx].transAxes)\n",
    "            ### reduce the size of x labels\n",
    "            axs[lvl_idx, info_idx].tick_params(axis='x', labelsize=10)\n",
    "            ### make x label intervals bracket_size\n",
    "            axs[lvl_idx, info_idx].set_xticks(bracketed_keys)\n",
    "            axs[lvl_idx, info_idx].set_xticklabels(bracketed_keys)\n",
    "            axs[lvl_idx, info_idx].set_xlabel(f\"{bar_type} coverage\")\n",
    "            \n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    return meaningful_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAIN\n",
    "def analyze_distribution(representation, tutorial_urls, url_to_nice_name):\n",
    "    stats = analysis_count_summary(\n",
    "        representation, tutorial_urls, url_to_nice_name,\n",
    "        # key_levels_to_draw=list(KEY_LEVELS.keys())[3:5],\n",
    "        key_levels_to_draw=list(KEY_LEVELS.keys()),\n",
    "        information_keys_to_draw=PRIMARY_INFORMATION_KEYS,\n",
    "    )\n",
    "\n",
    "    stats = analysis_count_summary(\n",
    "        representation, tutorial_urls, url_to_nice_name,\n",
    "        # key_levels_to_draw=list(KEY_LEVELS.keys())[3:5],\n",
    "        key_levels_to_draw=list(KEY_LEVELS.keys()),\n",
    "        information_keys_to_draw=SUPP_INFORMATION_KEYS,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import networkx as nx\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import MDS\n",
    "from umap import UMAP\n",
    "\n",
    "def extract_subset_tutorial_representation(representation, urls):\n",
    "    result = {}\n",
    "    for phase in representation:\n",
    "        result[phase] = {}\n",
    "        for step in representation[phase]:\n",
    "            result[phase][step] = {}\n",
    "            for ipo_key in representation[phase][step]:\n",
    "                result[phase][step][ipo_key] = {}\n",
    "                for element in representation[phase][step][ipo_key]:\n",
    "                    result[phase][step][ipo_key][element] = []\n",
    "                    reps = representation[phase][step][ipo_key][element]\n",
    "                    for rep in reps:\n",
    "                        should_append = False\n",
    "                        for item in rep[\"items\"]:\n",
    "                            if item[\"url\"] in urls:\n",
    "                                should_append = True\n",
    "                                break\n",
    "                        if should_append:\n",
    "                            result[phase][step][ipo_key][element].append(rep)\n",
    "    return result\n",
    "\n",
    "def extract_tutorial_representation(representation, url):\n",
    "    return extract_subset_tutorial_representation(representation, [url])\n",
    "\n",
    "def edit_distance(reps_1, reps_2, information_key):\n",
    "    contents_1 = []\n",
    "    contents_2 = []\n",
    "    for rep in reps_1:\n",
    "        if rep[\"type\"] == information_key:\n",
    "            contents_1.append(rep[\"content\"])\n",
    "    for rep in reps_2:\n",
    "        if rep[\"type\"] == information_key:\n",
    "            contents_2.append(rep[\"content\"])\n",
    "\n",
    "    ### remove dups\n",
    "    prev_len = len(contents_1)\n",
    "    contents_1 = list(set(contents_1))\n",
    "    if len(contents_1) != prev_len:\n",
    "        print(\"Found duplicate info pieces when calculating edit distance\")\n",
    "    contents_2 = list(set(contents_2))\n",
    "\n",
    "    ### edit distance\n",
    "    if len(contents_1) + len(contents_2) == 0:\n",
    "        return 0\n",
    "    common = 0\n",
    "    for c1 in contents_1:\n",
    "        for c2 in contents_2:\n",
    "            if c1 == c2:\n",
    "                common += 1\n",
    "    return len(contents_1) + len(contents_2) - 2 * common\n",
    "\n",
    "\n",
    "def tutorial_distance(t1_rep, t2_rep, weights_map):\n",
    "    distance = 0\n",
    "\n",
    "    for phase in t1_rep:\n",
    "        for step in t1_rep[phase]:\n",
    "            ### can try reflecting the importance of a step/phase but later;\n",
    "            for ipo_key in t1_rep[phase][step]:\n",
    "                ### can try different weights for input/method/output\n",
    "                for element in t1_rep[phase][step][ipo_key]:\n",
    "                    reps_1 = t1_rep[phase][step][ipo_key][element]\n",
    "                    reps_2 = t2_rep[phase][step][ipo_key][element]\n",
    "                    for information_key in PRIMARY_INFORMATION_KEYS:\n",
    "                        distance += edit_distance(reps_1, reps_2, information_key) * weights_map[\"primary\"]\n",
    "                    for information_key in SUPP_INFORMATION_KEYS:\n",
    "                        distance += edit_distance(reps_1, reps_2, information_key) * weights_map[\"supp\"]\n",
    "    return distance\n",
    "\n",
    "def tutorial_distance_matrix(representation, tutorial_urls, weights_map):\n",
    "    \"\"\"\n",
    "    calculate the distance matrix between each tutorial\n",
    "    \"\"\"\n",
    "\n",
    "    t_reps = []\n",
    "    for url in tutorial_urls:\n",
    "        t_rep = extract_tutorial_representation(representation, url)\n",
    "        t_reps.append(t_rep)\n",
    "\n",
    "    distance_matrix = []\n",
    "    for i in range(len(tutorial_urls)):\n",
    "        distance_matrix.append([0] * len(tutorial_urls))\n",
    "    \n",
    "    for i in range(len(tutorial_urls)):\n",
    "        for j in range(i + 1, len(tutorial_urls)):\n",
    "            distance = tutorial_distance(t_reps[i], t_reps[j], weights_map)\n",
    "            distance_matrix[i][j] = distance\n",
    "            distance_matrix[j][i] = distance\n",
    "    return distance_matrix\n",
    "\n",
    "def visualize_tutorial_comparison(representation, url_1, url_2, bar_chart_level=\"phase\", only_primary=True):\n",
    "    \"\"\"\n",
    "    compare the representations of two tutorials\n",
    "    \"\"\"\n",
    "    t1_rep = extract_tutorial_representation(representation, url_1)\n",
    "    t2_rep = extract_tutorial_representation(representation, url_2)\n",
    "\n",
    "    ### plot bar chart based on `bar_chart_level` (e.g., phase)\n",
    "    bar_chart_data = {}\n",
    "    diff = {}\n",
    "    for phase in representation:\n",
    "        diff[phase] = {}\n",
    "        for step in representation[phase]:\n",
    "            diff[phase][step] = {}\n",
    "            for ipo_key in representation[phase][step]:\n",
    "                diff[phase][step][ipo_key] = {}\n",
    "                for element in representation[phase][step][ipo_key]:\n",
    "\n",
    "                    reps = representation[phase][step][ipo_key][element]\n",
    "                    reps_1 = t1_rep[phase][step][ipo_key][element]\n",
    "                    reps_2 = t2_rep[phase][step][ipo_key][element]\n",
    "                    \n",
    "                    diff[phase][step][ipo_key][element] = {}\n",
    "\n",
    "                    for information_key in INFORMATION_KEYS:\n",
    "                        if only_primary is True and information_key not in PRIMARY_INFORMATION_KEYS:\n",
    "                            continue\n",
    "                        per_piece = {}\n",
    "                        \n",
    "                        for rep in reps:\n",
    "                            if rep[\"type\"] == information_key:\n",
    "                                per_piece[rep[\"content\"]] = 0\n",
    "                        for rep in reps_1:\n",
    "                            if rep[\"type\"] == information_key:\n",
    "                                per_piece[rep[\"content\"]] = 1\n",
    "                        for rep in reps_2:\n",
    "                            if rep[\"type\"] != information_key:\n",
    "                                continue\n",
    "                            if per_piece[rep[\"content\"]] == 1:\n",
    "                                per_piece[rep[\"content\"]] = 3\n",
    "                            else:    \n",
    "                                per_piece[rep[\"content\"]] = 2\n",
    "                        diff[phase][step][ipo_key][element][information_key] = per_piece\n",
    "\n",
    "                        bc_key = None\n",
    "                        if bar_chart_level == \"phase\":\n",
    "                            bc_key = phase\n",
    "                        if bar_chart_level == \"step\":\n",
    "                            bc_key = step\n",
    "                        if bar_chart_level == \"ipo\":\n",
    "                            bc_key = ipo_key\n",
    "                        if bar_chart_level == \"element\":\n",
    "                            bc_key = element\n",
    "                        if bar_chart_level == \"information\":\n",
    "                            bc_key = information_key\n",
    "                        if bc_key not in bar_chart_data:\n",
    "                            bar_chart_data[bc_key] = {}\n",
    "                        for piece in per_piece:\n",
    "                            cur_id = f\"{per_piece[piece]}\"\n",
    "                            if cur_id not in bar_chart_data[bc_key]:\n",
    "                                bar_chart_data[bc_key][cur_id] = 0\n",
    "                            bar_chart_data[bc_key][cur_id] += 1\n",
    "\n",
    "    ### draw a composition bar chart\n",
    "    # 0 -> black (absent from both),\n",
    "    # 1 -> blue (only in tutorial 1),\n",
    "    # 2 -> red (only in tutorial 2),\n",
    "    # 3 -> yellow (present in both)\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    keys = list(bar_chart_data.keys())\n",
    "    values = []\n",
    "    for key in keys:\n",
    "        zero = bar_chart_data[key]['0'] if '0' in bar_chart_data[key] else 0\n",
    "        one = bar_chart_data[key]['1'] if '1' in bar_chart_data[key] else 0\n",
    "        two = bar_chart_data[key]['2'] if '2' in bar_chart_data[key] else 0\n",
    "        three = bar_chart_data[key]['3'] if '3' in bar_chart_data[key] else 0\n",
    "        total = zero + one + two + three\n",
    "        zero /= total\n",
    "        one /= total\n",
    "        two /= total\n",
    "        three /= total\n",
    "        values.append([zero, one, two, three])\n",
    "    values = np.array(values)\n",
    "    bar_width = 0.35\n",
    "    x = np.arange(len(keys))\n",
    "    plt.bar(x, values[:, 3], width=bar_width, color='yellow', label='both')\n",
    "    plt.bar(x, values[:, 1], width=bar_width, color='blue', label='only in 1', bottom=values[:, 3])\n",
    "    plt.bar(x, values[:, 2], width=bar_width, color='red', label='only in 2', bottom=values[:, 3] + values[:, 1])\n",
    "    # plt.bar(x, values[:, 0], width=bar_width, color='black', label='0', bottom=values[:, 3] + values[:, 1] + values[:, 2])\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.xticks(x, keys, rotation=45)\n",
    "    plt.xlabel(bar_chart_level)\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    print()\n",
    "\n",
    "def visualize_tutorial_embeddings(distance_matrix, labels, labels_to_show, top_k=3, k_means_clusters_n=5, method=\"tsne\"):\n",
    "    \"\"\"\n",
    "    visualize the tutorials in 2D space based on the pairwise distances\n",
    "    distance_matrix: a matrix of size (n, n) where n is the number of tutorials and values are floats\n",
    "    \"\"\"\n",
    "    ## scale the distance matrix to range [0,1]\n",
    "    # Make symmetric by averaging with transpose\n",
    "    distance_matrix = np.array(distance_matrix)\n",
    "    distance_matrix = (distance_matrix + distance_matrix.T) / 2\n",
    "    \n",
    "    # Scale distances to [0,1] range\n",
    "    max_dist = np.max(distance_matrix)\n",
    "    if max_dist > 0:  # Avoid division by zero\n",
    "        scaled_distances = distance_matrix / max_dist\n",
    "    else:\n",
    "        scaled_distances = distance_matrix\n",
    "\n",
    "    result = []\n",
    "    # Perform t-SNE\n",
    "    if method == \"tsne\":\n",
    "        tsne = TSNE(n_components=2, random_state=42, method=\"exact\",)\n",
    "        result = tsne.fit_transform(scaled_distances)\n",
    "\n",
    "    # Perform UMAP\n",
    "    if method == \"umap\":\n",
    "        umap = UMAP(n_components=2, random_state=42, metric=\"precomputed\")\n",
    "        result = umap.fit_transform(scaled_distances)\n",
    "    \n",
    "    # Perform MDS\n",
    "    if method == \"mds\":\n",
    "        mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=42)\n",
    "        result = mds.fit_transform(distance_matrix)\n",
    "\n",
    "    # k-means clustering\n",
    "    kmeans = KMeans(n_clusters=k_means_clusters_n, random_state=42)\n",
    "    kmeans.fit(result)\n",
    "    k_means_labels = kmeans.labels_\n",
    "    colors = matplotlib.colormaps.get_cmap(\"tab10\")\n",
    "    \n",
    "    clusters = {}\n",
    "    for i in range(len(result)):\n",
    "        if k_means_labels[i] not in clusters:\n",
    "            clusters[k_means_labels[i]] = []\n",
    "        clusters[k_means_labels[i]].append(i)\n",
    "    for cluster in clusters:\n",
    "        print(f\"Cluster {cluster}: {clusters[cluster]}\")\n",
    "\n",
    "    ## visualize\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.title(f\"{method}\")\n",
    "    for i in range(len(result)):\n",
    "        plt.scatter(result[i][0], result[i][1], color=colors(k_means_labels[i]))\n",
    "        if i in labels_to_show and len(labels_to_show) > 0:\n",
    "            plt.annotate(i, (result[i][0], result[i][1]), fontsize=8)\n",
    "    plt.legend(loc=\"upper right\", fontsize=8)\n",
    "    plt.show()\n",
    "\n",
    "    def eucledian(a, b):\n",
    "        return np.sqrt(np.sum((a - b) ** 2))\n",
    "\n",
    "    comparison_candidates = []\n",
    "    ### print k closest tutorials\n",
    "    for i in range(len(result)):\n",
    "        if i not in labels_to_show:\n",
    "            continue\n",
    "        distances = []\n",
    "        for j in range(len(result)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            distances.append((j, eucledian(result[i], result[j])))\n",
    "        distances = sorted(distances, key=lambda x: x[1])\n",
    "        pick = [(0, 'closest'), (-1, 'farthest')]\n",
    "        # pick = [(6, \"7th closest\"), (8, '9th closest'), (9, '10th closest'), (10, 'farthest')]\n",
    "        for idx, label in pick:\n",
    "            j = distances[idx][0]\n",
    "            dist = distances[idx][1]\n",
    "            comparison_candidates.append((i, j, label, dist, result[i], result[j]))\n",
    "    if top_k == -1:\n",
    "        return comparison_candidates\n",
    "    comparison_candidates = sorted(comparison_candidates, key=lambda x: x[3])\n",
    "    if len(comparison_candidates) < top_k * 2:\n",
    "        return comparison_candidates\n",
    "    return comparison_candidates[0:top_k] + comparison_candidates[-top_k:]\n",
    "\n",
    "\n",
    "def visualize_representation(representation, information_keys, tutorial_urls, show_all=False, optimize_sum_of_edges=False):\n",
    "    cooc = []\n",
    "    count_labels = []\n",
    "    for pidx, phase in enumerate(representation.keys()):\n",
    "        for sidx, step in enumerate(representation[phase].keys()):\n",
    "            for ipo in [\"inputs\", \"methods\", \"outputs\"]:\n",
    "                for eidx, element in enumerate(representation[phase][step][ipo].keys()):\n",
    "                    coverage = {}\n",
    "                    for url in tutorial_urls:\n",
    "                        coverage[url] = 0\n",
    "                    cur_count_labels = []\n",
    "                    for ridx, rep in enumerate(representation[phase][step][ipo][element]):\n",
    "                        if rep[\"type\"] not in information_keys:\n",
    "                            continue\n",
    "                        # for item in rep[\"items\"]:\n",
    "                        #     count_labels.append({\n",
    "                        #         \"idx\": len(count_labels),\n",
    "                        #         \"phase\": phase,\n",
    "                        #         \"step\": step,\n",
    "                        #         \"ipo\": ipo,\n",
    "                        #         \"element\": element,\n",
    "                        #         \"type\": rep[\"type\"],\n",
    "                        #         \"urls\": [item[\"url\"]],\n",
    "                        #         \"content\": item[\"content\"]\n",
    "                        #     })\n",
    "                        cur_urls = []\n",
    "                        for item in rep[\"items\"]:\n",
    "                            if item['url'] in tutorial_urls:\n",
    "                                cur_urls.append(item['url'])\n",
    "                                coverage[item['url']] += 1\n",
    "                        if len(cur_urls) == 0 and show_all is False:\n",
    "                            continue\n",
    "                        cur_idx = f\"{pidx}-{sidx}-{ipo[0]}-{eidx}-{ridx}\"\n",
    "                        cur_count_labels.append({\n",
    "                            \"idx\": cur_idx,\n",
    "                            \"phase\": phase,\n",
    "                            \"step\": step,\n",
    "                            \"ipo\": ipo,\n",
    "                            \"element\": element,\n",
    "                            \"type\": rep[\"type\"],\n",
    "                            \"urls\": cur_urls,\n",
    "                            \"content\": rep[\"content\"]\n",
    "                        })\n",
    "                    if len(cur_count_labels) == 0:\n",
    "                        continue\n",
    "                    empty_urls = []\n",
    "                    for url in coverage.keys():\n",
    "                        if coverage[url] > 1:\n",
    "                            print(\"Error: multiple representations for the same url:\", url)\n",
    "                        if coverage[url] == 0:\n",
    "                            empty_urls.append(url)\n",
    "                    cur_idx = f\"{pidx}-{sidx}-{ipo[0]}-{eidx}-e\"\n",
    "                    count_labels.append({\n",
    "                        \"idx\": cur_idx,\n",
    "                        \"phase\": phase,\n",
    "                        \"step\": step,\n",
    "                        \"ipo\": ipo,\n",
    "                        \"element\": element,\n",
    "                        \"type\": \"empty\",\n",
    "                        \"urls\": empty_urls,\n",
    "                        \"content\": \"<empty>\"\n",
    "                    })\n",
    "                    count_labels.extend(cur_count_labels)\n",
    "            # ## todo: remove this\n",
    "            # break\n",
    "        # ### todo: remove this\n",
    "        # break\n",
    "    \n",
    "    per_element_cooc = {}\n",
    "    per_element_layer_ids = {}\n",
    "    for l in count_labels:\n",
    "        label = f\"{l['phase']}-{l['step']}-{l['ipo']}-{l['element']}\"\n",
    "        if label not in per_element_cooc:\n",
    "            per_element_cooc[label] = []\n",
    "            per_element_layer_ids[label] = len(per_element_layer_ids)\n",
    "        per_element_cooc[label].append(l)\n",
    "        l[\"layer\"] = per_element_layer_ids[label]\n",
    "        \n",
    "    for l1 in count_labels:\n",
    "        cur_cooc = []\n",
    "        for l2 in count_labels:\n",
    "            if l1[\"idx\"] == l2[\"idx\"]:\n",
    "                cur_cooc.append((0, \"\"))\n",
    "                continue\n",
    "            ### continue if layers are not adjacent\n",
    "            if l1[\"layer\"] != l2[\"layer\"] - 1:\n",
    "                cur_cooc.append((0, \"\"))\n",
    "                continue\n",
    "            ### co-occurences = # of urls in both l1 and l2\n",
    "            cur = 0\n",
    "            url_concat = \"\"\n",
    "            for url in l1[\"urls\"]:\n",
    "                if url in l2[\"urls\"]:\n",
    "                    cur += 1\n",
    "                    url_concat += f\"{url.split('=')[-1]}+\"\n",
    "            cur_cooc.append((cur, url_concat))\n",
    "        cooc.append(cur_cooc)\n",
    "\n",
    "\n",
    "    ### TODO: optimize the sum of edges (i.e., technically we only preserve the edges between adjacent layers, so we need to choose the most optimal arrangement of layers to maximize the sum of edges). \n",
    "    ### one restriction is that we can only swap layers that belong to the same `phase`-`step`-`ipo` and global arrangement of layer blocks should be preserved.\n",
    "    ### we can brute force all possible arrangements since the number of permutations within a phase-step-ipo group is at most 6 (24)\n",
    "\n",
    "\n",
    "    node_color_map = {\n",
    "        \"inputs\": (0, 0.5, 0), # green\n",
    "        \"methods\": (0, 0, 0.5), # blue\n",
    "        \"outputs\": (0.5, 0, 0), # red\n",
    "        \"empty\": (0.5, 0.5, 0.5), # gray\n",
    "    }\n",
    "\n",
    "    plt.figure(figsize=(150, 20))\n",
    "    G = nx.Graph()\n",
    "    for element_label in per_element_cooc.keys():\n",
    "        x = per_element_layer_ids[element_label]\n",
    "        for y, l in enumerate(per_element_cooc[element_label]):\n",
    "            ratio_of_urls = len(l[\"urls\"]) / len(tutorial_urls)\n",
    "            label = f\"{l['ipo'][0]}-{l['element']}-{l['content']}\"\n",
    "            node_color = node_color_map[l['ipo']]\n",
    "            if l['type'] == \"empty\":\n",
    "                node_color = node_color_map[\"empty\"]\n",
    "            label_dis = \"\".join(l[\"idx\"].split(\"-\")[-3:])\n",
    "            G.add_node(\n",
    "                l[\"idx\"], \n",
    "                label=label, \n",
    "                pos=(x, y), \n",
    "                label_dis=label_dis, \n",
    "                node_color=node_color, \n",
    "                # node_linewidth=ratio_of_urls, \n",
    "                node_size=ratio_of_urls*2000,\n",
    "                percentage=ratio_of_urls*100\n",
    "            )\n",
    "            ### write the `percentage` at the top of the node\n",
    "            plt.text(x, y + 0.1, f\"{(ratio_of_urls*100):.1f}%\", fontsize=10, ha='center', va='center')\n",
    "        ### write text at (x, -1), adaptively wrap the text if too long\n",
    "        lines = element_label.split(\"-\")\n",
    "        plt.text(x, -0.8, lines[0], fontsize=7, ha='center', va='center')\n",
    "        plt.text(x, -0.6, lines[1], fontsize=7, ha='center', va='center')\n",
    "        plt.text(x, -0.4, lines[2], fontsize=10, ha='center', va='center')\n",
    "        plt.text(x, -0.2, lines[3], fontsize=7, ha='center', va='center')\n",
    "    \n",
    "    max_weight = 0\n",
    "    for i in range(len(cooc)):\n",
    "        for j in range(i + 1, len(cooc)):\n",
    "            if cooc[i][j][0] > 0:\n",
    "                G.add_edge(\n",
    "                    count_labels[i][\"idx\"],\n",
    "                    count_labels[j][\"idx\"],\n",
    "                    weight=cooc[i][j][0],\n",
    "                    label_dis=f\"{cooc[i][j][0]}\",\n",
    "                    label=f\"{cooc[i][j][0]/len(tutorial_urls)*100:.1f}%\",\n",
    "                )\n",
    "                max_weight = max(max_weight, cooc[i][j][0])\n",
    "    \n",
    "    ### adjust edge opacity based on weight\n",
    "    for i, j, data in G.edges(data=True):\n",
    "        data[\"label_dis\"] = f\"{data['label']}\"\n",
    "        data['width'] = (data['weight']-1) / max_weight * 10 + 1\n",
    "        \n",
    "    nx.draw(G,\n",
    "        pos=nx.get_node_attributes(G, 'pos'),\n",
    "        with_labels=True,\n",
    "        labels=nx.get_node_attributes(G, 'label_dis'),\n",
    "        node_color=list(nx.get_node_attributes(G, 'node_color').values()),\n",
    "        width=list(nx.get_edge_attributes(G, 'width').values()),\n",
    "        node_size=list(nx.get_node_attributes(G, 'node_size').values()),\n",
    "        # node_linewidth=list(nx.get_node_attributes(G, 'node_linewidth').values())\n",
    "    )\n",
    "    # nx.draw_networkx_edge_labels(G,\n",
    "    #     edge_labels=nx.get_edge_attributes(G, 'label_dis'),\n",
    "    #     pos=nx.get_node_attributes(G, 'pos'),\n",
    "    # )\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAIN\n",
    "### Visualization Experiments\n",
    "\n",
    "def visualization_experiments(representation, tutorial_urls):\n",
    "    weights_map = {\n",
    "        ### info\n",
    "        \"primary\": 1, ### edit_distance: edits / (len(t1) + len(t2))\n",
    "        \"supp\": 0, ### edit distance: edits / (len(t1) + len(t2))\n",
    "    }\n",
    "\n",
    "    distance_matrix = tutorial_distance_matrix(representation, tutorial_urls, weights_map)\n",
    "    to_show = [31, 42, 46, 47, 63, 91, 92]\n",
    "    # empties = [16, 17, 21, 23, 32, 48, 50, 51, 57, 93]\n",
    "    # close_to_empty = [31, 42, 46, 47, 63, 91, 92]\n",
    "    # to_show = [8, 13, 14, 41, 65, 69, 73, 77]\n",
    "    # to_show = [0, 10, 15, 22, 29, 36, 52, 58, 74, 86, 95, 97]\n",
    "    # to_show = [1, 4, 26, 44, 55, 56, 60, 61, 75, 85, 89, 96, 98]\n",
    "    # to_show = [2, 6, 11, 12, 40, 53, 64, 67, 68, 79, 90, 94]\n",
    "    # to_show = [7, 19, 33, 34, 35, 38, 39, 62, 66, 71, 72, 78, 80, 83, 84]\n",
    "    # far_from_empty = [3, 9, 18, 20, 24, 27, 37, 43, 45, 49, 54, 70, 82, 87]\n",
    "\n",
    "    n_clusters = 3\n",
    "    comparison_pairs = []\n",
    "    # comparison_pairs = visualize_tutorials(distance_matrix, tutorial_urls, to_show, -1, n_clusters, method=\"tsne\")\n",
    "    # comparison_pairs = visualize_tutorials(distance_matrix, tutorial_urls, to_show, -1, n_clusters, method=\"mds\")\n",
    "    # comparison_pairs = visualize_tutorials(distance_matrix, tutorial_urls, to_show, -1, n_clusters, method=\"umap\")\n",
    "\n",
    "    # close_to_empty = [31, 42, 46, 47, 63, 91, 92]\n",
    "    # compare_subset_tutorial_representations(representation, [tutorial_urls[i] for i in to_show_1], \"step\", True)\n",
    "\n",
    "    # subset_rep = extract_subset_tutorial_representation(representation, [tutorial_urls[i] for i in to_show])\n",
    "    # print(json.dumps(subset_rep, indent=4))\n",
    "\n",
    "    # for pair in comparison_pairs[:10]:\n",
    "    #     t1 = tutorial_urls[pair[0]]\n",
    "    #     t2 = tutorial_urls[pair[1]]\n",
    "    #     kind = pair[2]\n",
    "    #     dist = pair[3]\n",
    "    #     pos1 = pair[4]\n",
    "    #     pos2 = pair[5]\n",
    "    #     print (f\"Comparing {pair[0]} and {pair[1]} ({kind}) with distance {dist} at position {pos1} and {pos2}\")\n",
    "    #     compare_tutorial_representations(representation, t1, t2, \"element\", True)\n",
    "\n",
    "    visualize_representation(representation, PRIMARY_INFORMATION_KEYS, tutorial_urls)\n",
    "    # # visualize_representation(representation, [PRIMARY_INFORMATION_KEYS[0]], tutorial_urls)\n",
    "    # visualize_representation(representation, [PRIMARY_INFORMATION_KEYS[1]], tutorial_urls)\n",
    "\n",
    "    # for url in tutorial_urls:\n",
    "    #     print(f\"showing {url}\")\n",
    "    #     visualize_representation(representation, PRIMARY_INFORMATION_KEYS, [url], show_all=True)\n",
    "\n",
    "    # for i in range(len(tutorial_urls)):\n",
    "    #     for j in range(i + 1, len(tutorial_urls)):\n",
    "    #         print(f\"Comparing {tutorial_urls[i]} and {tutorial_urls[j]}\")\n",
    "    #         visualize_representation(representation, PRIMARY_INFORMATION_KEYS, [tutorial_urls[i], tutorial_urls[j]], show_all=True)\n",
    "    #     break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fragmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.bert import bert_embedding\n",
    "\n",
    "def print_md_table(data):\n",
    "    \"\"\"\n",
    "    data is a dict of form fragmentation[focus][distance_type]\n",
    "    \"\"\"\n",
    "    print(\"| Focus | Fine | Coarse | Breadth |\")\n",
    "    print(\"|-------|------|--------|--------|\")\n",
    "    for focus in data:\n",
    "        fine = data[focus]['fine']\n",
    "        coarse = data[focus]['coarse']\n",
    "        breadth = \"\"\n",
    "        if \"breadth\" in data[focus]:\n",
    "            breadth = data[focus][\"breadth\"]\n",
    "        print(f\"| {focus} | {fine:.2f} | {coarse:.2f} | {breadth}\")\n",
    "\n",
    "def calc_fragmentation_coarse(representation, tutorial_urls, ipo_weights):\n",
    "    pass\n",
    "\n",
    "def calc_fragmentation_pairwise(representation, tutorial_urls, ipo_weights, information_keys, distance_type=\"fine\"):\n",
    "\n",
    "    def calc_option_distance(option_1, option_2):\n",
    "        if distance_type == \"fine\":\n",
    "            return np.dot(option_1, option_2)\n",
    "        elif distance_type == \"coarse\":\n",
    "            if option_1 == option_2:\n",
    "                return 0\n",
    "            else:\n",
    "                return 1\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid distance type: {distance_type}\")\n",
    "\n",
    "    total = 0\n",
    "    fragmentation = 0\n",
    "    for phase in representation:\n",
    "        for step in representation[phase]:\n",
    "            for ipo in representation[phase][step]:\n",
    "                for element in representation[phase][step][ipo]:\n",
    "                    options = {}\n",
    "                    for tutorial_url in tutorial_urls:\n",
    "                        options[tutorial_url] = \"\"\n",
    "                    for rep in representation[phase][step][ipo][element]:\n",
    "                        if rep[\"type\"] in information_keys:\n",
    "                            for item in rep[\"items\"]:\n",
    "                                if distance_type == \"fine\":\n",
    "                                    options[item[\"url\"]] = item[\"content\"]\n",
    "                                elif distance_type == \"coarse\":\n",
    "                                    options[item[\"url\"]] = rep[\"content\"]\n",
    "                                else:\n",
    "                                    raise ValueError(f\"Invalid distance type: {distance_type}\")\n",
    "                    if distance_type == \"fine\":\n",
    "                        for ti in options.keys():\n",
    "                            options[ti] = bert_embedding([options[ti]])[0]\n",
    "                    ### calculate pairwise distances\n",
    "                    for ti in options.keys():\n",
    "                        for tj in options.keys():\n",
    "                            if ti == tj:\n",
    "                                continue\n",
    "                            dist = calc_option_distance(options[ti], options[tj])\n",
    "                            total += ipo_weights[ipo]\n",
    "                            fragmentation += dist * ipo_weights[ipo]\n",
    "    return fragmentation / total\n",
    "\n",
    "def calc_breadth(representation, tutorial_urls, ipo_weights, information_keys):\n",
    "    per_step_breadth = {}\n",
    "    for phase in representation:\n",
    "        for step in representation[phase]:\n",
    "            if step not in per_step_breadth:\n",
    "                per_step_breadth[step] = 1\n",
    "            for ipo in representation[phase][step]:\n",
    "                if ipo_weights[ipo] < 1:\n",
    "                    continue\n",
    "                for element in representation[phase][step][ipo]:\n",
    "                    options = {}\n",
    "                    for tutorial_url in tutorial_urls:\n",
    "                        options[tutorial_url] = \"\"\n",
    "                    cur_breadth = len(representation[phase][step][ipo][element])\n",
    "                    for rep in representation[phase][step][ipo][element]:\n",
    "                        if rep[\"type\"] in information_keys:\n",
    "                            for item in rep[\"items\"]:\n",
    "                                options[item[\"url\"]] = rep[\"content\"]\n",
    "                    for ti in options.keys():\n",
    "                        if options[ti] == \"\":\n",
    "                            cur_breadth += 1\n",
    "                    per_step_breadth[step] *= cur_breadth\n",
    "\n",
    "    vals = np.array(list(per_step_breadth.values()))\n",
    "    avg = np.average(vals)\n",
    "    std = np.std(vals)\n",
    "    return avg, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAIN\n",
    "def analyze_fragmentation(representation, tutorial_urls):\n",
    "    proc_fragmentation = {}\n",
    "    for i_weights in [0, 1]:\n",
    "        for m_weights in [0, 1]:\n",
    "            for o_weights in [0, 1]:\n",
    "                ipo_weights = {\n",
    "                    \"inputs\": i_weights,\n",
    "                    \"methods\": m_weights,\n",
    "                    \"outputs\": o_weights,\n",
    "                }\n",
    "                focus = \"\"\n",
    "                if i_weights == 1:\n",
    "                    focus += \"inputs-\"\n",
    "                if m_weights == 1:\n",
    "                    focus += \"methods-\"\n",
    "                if o_weights == 1:\n",
    "                    focus += \"outputs-\"\n",
    "                if focus == \"\":\n",
    "                    continue\n",
    "                if focus not in proc_fragmentation:\n",
    "                    proc_fragmentation[focus] = {}\n",
    "                for distance_type in [\"fine\", \"coarse\"]:\n",
    "                    fragmentation = calc_fragmentation_pairwise(representation, tutorial_urls, ipo_weights, PRIMARY_INFORMATION_KEYS, distance_type)\n",
    "                    proc_fragmentation[focus][distance_type] = fragmentation\n",
    "\n",
    "    print(json.dumps(proc_fragmentation, indent=4))\n",
    "\n",
    "    info_fragmentation = {}\n",
    "    for information_keys in [SUPP_INFORMATION_KEYS, [SUPP_INFORMATION_KEYS[0]], [SUPP_INFORMATION_KEYS[1]]]:\n",
    "        ipo_weights = {\n",
    "            \"inputs\": 0,\n",
    "            \"methods\": 1,\n",
    "            \"outputs\": 0,\n",
    "        }\n",
    "        focus = \"\"\n",
    "        for i in information_keys:\n",
    "            focus += i + \"-\"\n",
    "        if focus == \"\":\n",
    "            continue\n",
    "        if focus not in info_fragmentation:\n",
    "            info_fragmentation[focus] = {}\n",
    "        for distance_type in [\"fine\", \"coarse\"]:\n",
    "            fragmentation = calc_fragmentation_pairwise(representation, tutorial_urls, ipo_weights, information_keys, distance_type)\n",
    "            info_fragmentation[focus][distance_type] = fragmentation\n",
    "\n",
    "    print(json.dumps(info_fragmentation, indent=4))\n",
    "\n",
    "    for i_weights in [0, 1]:\n",
    "        for m_weights in [0, 1]:\n",
    "            for o_weights in [0, 1]:\n",
    "                ipo_weights = {\n",
    "                    \"inputs\": i_weights,\n",
    "                    \"methods\": m_weights,\n",
    "                    \"outputs\": o_weights,\n",
    "                }\n",
    "                focus = \"\"\n",
    "                if i_weights == 1:\n",
    "                    focus += \"inputs-\"\n",
    "                if m_weights == 1:\n",
    "                    focus += \"methods-\"\n",
    "                if o_weights == 1:\n",
    "                    focus += \"outputs-\"\n",
    "                if focus == \"\":\n",
    "                    continue\n",
    "                if focus not in proc_fragmentation:\n",
    "                    proc_fragmentation[focus] = {}\n",
    "                avg, std = calc_breadth(representation, tutorial_urls, ipo_weights, PRIMARY_INFORMATION_KEYS)\n",
    "                proc_fragmentation[focus][\"breadth\"] = f\"avg={avg:.2f}, std={std:.2f}\"\n",
    "\n",
    "    print_md_table(proc_fragmentation)\n",
    "    print()\n",
    "    print_md_table(info_fragmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MUFFIN_TASK = \"Making Muffins\"\n",
    "\n",
    "\"\"\"\n",
    "Make French Toast\t\t\t10 steps / 272 videos\n",
    "Make Irish Coffee\t\t\t5 steps / 248 videos\n",
    "Change a Tire\t\t\t\t11 steps / 119 videos\n",
    "Build (sim.) Floating Shelves\t\t5 steps / 173 videos\n",
    "\n",
    "\"\"\"\n",
    "CROSS_TASK_TASKS = [\n",
    "    \"Change a Tire\",\n",
    "    \"Build (sim.) Floating Shelves\",\n",
    "    \"Make French Toast\",\n",
    "    \"Make Irish Coffee\",\n",
    "]\n",
    "\n",
    "def get_dataset(task):\n",
    "    if task == MUFFIN_TASK:\n",
    "        return get_dataset_muffins(task, \"raw\")\n",
    "    else:\n",
    "        return preprocess_cross_task(task, \"raw\")\n",
    "\n",
    "def main(task, agg_approach=\"llm-based\", agg_distance_threshold=0.3):\n",
    "    dataset = get_dataset(task)\n",
    "    taxonomy = construct_step_taxonomy(dataset, task, \"stupid\")\n",
    "    dataset = extract_ipos_stupid(dataset, taxonomy, task, \"stupid\")\n",
    "    ipo_taxonomy = taxonomize_ipos(dataset, taxonomy, task, \"stupid\")\n",
    "    dataset = extract_information_per_ipo(dataset, taxonomy, ipo_taxonomy, task, \"stupid\")\n",
    "    representation = extract_task_representation(\n",
    "        dataset, taxonomy, ipo_taxonomy, task,\n",
    "        agg_approach, agg_distance_threshold,\n",
    "        \"stupid-\" + agg_approach\n",
    "    )\n",
    "\n",
    "    print(\"pre-process done!\")\n",
    "\n",
    "    ## todo: remove (since don't want to rerun the entire representation building again)\n",
    "    for phase in representation:\n",
    "        for step in representation[phase]:\n",
    "            for ipo_key in representation[phase][step]:\n",
    "                for element in representation[phase][step][ipo_key]:\n",
    "                    representation[phase][step][ipo_key][element] = sorted(representation[phase][step][ipo_key][element], key=lambda x: len(x[\"items\"]), reverse=True)\n",
    "    ### end-todo\n",
    "\n",
    "    url_to_content = {}\n",
    "    url_to_nice_name = {}\n",
    "    for idx, article in enumerate(dataset):\n",
    "        url_to_content[article[\"url\"]] = article[\"content\"]\n",
    "        url_to_nice_name[article[\"url\"]] = f\"T{idx}\"\n",
    "\n",
    "    tutorial_urls = list(url_to_content.keys())\n",
    "    return representation, tutorial_urls, url_to_nice_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-process done!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "task = MUFFIN_TASK\n",
    "# task = CROSS_TASK_TASKS[0]\n",
    "# task = CROSS_TASK_TASKS[1]\n",
    "\n",
    "representation, tutorial_urls, url_to_nice_name = main(task, agg_approach=\"llm-based\")\n",
    "# representation, tutorial_urls, url_to_nice_name = main(task, agg_approach=\"clustering\", agg_distance_threshold=0.3)\n",
    "\n",
    "# analyze_distribution(representation, tutorial_urls, url_to_nice_name)\n",
    "# analyze_fragmentation(representation, tutorial_urls)\n",
    "# visualization_experiments(representation, tutorial_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### sample subgoal-ipo pairs for analysis\n",
    "subgoal_list = {}\n",
    "element_list = {}\n",
    "for phase in representation:\n",
    "    for step in representation[phase]:\n",
    "        subgoal_list[step] = {}\n",
    "        for ipo in representation[phase][step]:\n",
    "            if ipo not in subgoal_list[step]:\n",
    "                subgoal_list[step][ipo] = {}\n",
    "            \n",
    "            if ipo not in element_list:\n",
    "                element_list[ipo] = []\n",
    "            for element in representation[phase][step][ipo]:\n",
    "                element_list[ipo].append([phase, step, element, representation[phase][step][ipo][element]])\n",
    "                \n",
    "                for entry in representation[phase][step][ipo][element]:\n",
    "                    if entry[\"type\"] not in subgoal_list[step][ipo]:\n",
    "                        subgoal_list[step][ipo][entry[\"type\"]] = {}\n",
    "                    \n",
    "                    if element not in subgoal_list[step][ipo][entry[\"type\"]]:\n",
    "                        subgoal_list[step][ipo][entry[\"type\"]][element] = []\n",
    "                    subgoal_list[step][ipo][entry[\"type\"]][element].append(entry[\"content\"] + f\" ({len(entry['items'])})\")\n",
    "\n",
    "### sample 3 steps\n",
    "selected_steps = random.sample(list(subgoal_list.keys()), 3)\n",
    "csv_lines = []\n",
    "max_info_len = 0\n",
    "for step in selected_steps:\n",
    "    for ipo in subgoal_list[step]:\n",
    "        for info_type in subgoal_list[step][ipo]:\n",
    "            for element in subgoal_list[step][ipo][info_type]:\n",
    "                csv_lines.append([step, element, f\"{ipo} / {info_type}\"])\n",
    "                for info in subgoal_list[step][ipo][info_type][element]:\n",
    "                    csv_lines[-1].append(info)\n",
    "                max_info_len = max(max_info_len, len(csv_lines[-1]))\n",
    "\n",
    "csv_output_path = RESULTS_PATH + f\"qual_subgoal_list_{task}.csv\"\n",
    "with open(csv_output_path, \"w\") as f:\n",
    "    col_titles = [\"Step\", \"Class\", \"Ipo / Info Type\"]\n",
    "    while len(col_titles) < max_info_len:\n",
    "        col_titles.append(f\"Item {len(col_titles) - 3}\")\n",
    "    f.write(\",\".join(col_titles) + \"\\n\")\n",
    "    for line in csv_lines:\n",
    "        if len(line) < max_info_len:\n",
    "            line += [\"\"] * (max_info_len - len(line))\n",
    "        f.write(\",\".join([\"\\\"\" + x + \"\\\"\" for x in line]) + \"\\n\")\n",
    "\n",
    "\n",
    "csv_lines = []\n",
    "output = []\n",
    "for ipo in element_list:\n",
    "    selected = random.sample(element_list[ipo], 3)\n",
    "    for selected_element in selected:\n",
    "        prefix = [selected_element[1], selected_element[2]]\n",
    "        for entry in selected_element[3]:\n",
    "            csv_lines.append(prefix + [f\"{ipo} / {entry['type']}\", entry['content']])\n",
    "            for item in entry['items']:\n",
    "                csv_lines[-1].append(item['content'])\n",
    "            max_info_len = max(max_info_len, len(csv_lines[-1]))\n",
    "\n",
    "csv_output_path = RESULTS_PATH + f\"qual_element_list_{task}.csv\"\n",
    "with open(csv_output_path, \"w\") as f:\n",
    "    col_titles = [\"Step\", \"Class\", \"Ipo / Info Type\", \"Item\"]\n",
    "    while len(col_titles) < max_info_len:\n",
    "        col_titles.append(f\"Info {len(col_titles) - 4}\")\n",
    "    f.write(\",\".join(col_titles) + \"\\n\")\n",
    "    for line in csv_lines:\n",
    "        if len(line) < max_info_len:\n",
    "            line += [\"\"] * (max_info_len - len(line))\n",
    "        f.write(\",\".join([\"\\\"\" + x + \"\\\"\" for x in line]) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "starlab-video-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
